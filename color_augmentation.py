# color_augmentation.py
# é¢œè‰²ç‰¹å®šçš„æ•°æ®å¢å¼ºæ¨¡å—

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
import cv2
from PIL import Image
import random
from typing import Tuple, Dict
import math

class AdvancedColorSpecificAugmentation:
    """é«˜çº§é¢œè‰²ç‰¹å®šæ•°æ®å¢å¼º - ä¿æŒè¯­ä¹‰çš„åŒæ—¶å¢å¼ºé¢œè‰²é²æ£’æ€§"""
    
    def __init__(self, 
                 brightness_range=(0.6, 1.4),
                 contrast_range=(0.7, 1.3),
                 saturation_range=(0.5, 1.5),
                 hue_range=(-0.15, 0.15),
                 gamma_range=(0.6, 1.4),
                 color_shift_prob=0.6,
                 lighting_prob=0.4,
                 semantic_preserve_prob=0.7):
        
        self.brightness_range = brightness_range
        self.contrast_range = contrast_range
        self.saturation_range = saturation_range
        self.hue_range = hue_range
        self.gamma_range = gamma_range
        self.color_shift_prob = color_shift_prob
        self.lighting_prob = lighting_prob
        self.semantic_preserve_prob = semantic_preserve_prob
        
        # æ ‡å‡†é¢œè‰²å¢å¼º
        self.color_jitter = transforms.ColorJitter(
            brightness=brightness_range[1] - 1.0,
            contrast=contrast_range[1] - 1.0,
            saturation=saturation_range[1] - 1.0,
            hue=hue_range[1]
        )
        
        # é¢œè‰²ä¸å˜æ€§å¢å¼ºå™¨
        self.color_invariance_augmentor = ColorInvarianceAugmentor()
        
        # è¯­ä¹‰ä¿æŒå¢å¼ºå™¨
        self.semantic_preserve_augmentor = SemanticPreserveAugmentor()
        
        # å¯¹æŠ—æ€§é¢œè‰²å¢å¼ºå™¨
        self.adversarial_color_augmentor = AdversarialColorAugmentor()
    
    def __call__(self, image: torch.Tensor, targets: torch.Tensor = None) -> torch.Tensor:
        """åº”ç”¨é«˜çº§é¢œè‰²å¢å¼º"""
        if isinstance(image, torch.Tensor):
            if image.dim() == 4:  # æ‰¹æ¬¡ç»´åº¦
                augmented_images = []
                for i in range(image.size(0)):
                    target = targets[i] if targets is not None else None
                    img = self._augment_single_image(image[i], target)
                    augmented_images.append(img)
                return torch.stack(augmented_images)
            else:
                return self._augment_single_image(image, targets)
        else:
            return self._augment_single_image(image, targets)
    
    def _augment_single_image(self, image: torch.Tensor, target: int = None) -> torch.Tensor:
        """å¢å¼ºå•ä¸ªå›¾åƒ"""
        # è½¬æ¢ä¸ºPILå›¾åƒ
        if image.dim() == 3:
            img_array = image.permute(1, 2, 0).cpu().numpy()
            img_array = (img_array * 255).astype(np.uint8)
            pil_image = Image.fromarray(img_array)
        else:
            pil_image = image
        
        # 1. åŸºç¡€é¢œè‰²å¢å¼º
        if random.random() < self.color_shift_prob:
            pil_image = self.color_jitter(pil_image)
        
        # 2. å…‰ç…§å˜åŒ–
        if random.random() < self.lighting_prob:
            pil_image = self._adjust_lighting(pil_image)
        
        # 3. å½¢çŠ¶ä¿æŒçš„é¢œè‰²å¢å¼º
        if random.random() < 0.4:
            pil_image = self._shape_preserving_color_augmentation(pil_image)
        
        # 4. é¢œè‰²ä¸å˜æ€§å¢å¼º
        if random.random() < 0.5:
            pil_image = self.color_invariance_augmentor(pil_image)
        
        # 5. è¯­ä¹‰ä¿æŒå¢å¼º
        if random.random() < self.semantic_preserve_prob and target is not None:
            pil_image = self.semantic_preserve_augmentor(pil_image, target)
        
        # 6. å¯¹æŠ—æ€§é¢œè‰²å¢å¼º
        if random.random() < 0.3:
            pil_image = self.adversarial_color_augmentor(pil_image)
        
        # 7. è‡ªé€‚åº”é¢œè‰²å¢å¼º
        if random.random() < 0.4:
            pil_image = self._adaptive_color_augmentation(pil_image, target)
        
        # è½¬æ¢å›tensor
        if isinstance(pil_image, Image.Image):
            img_array = np.array(pil_image).astype(np.float32) / 255.0
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)
        else:
            img_tensor = pil_image
        
        return img_tensor
    
    def _adjust_lighting(self, image: Image.Image) -> Image.Image:
        """è°ƒæ•´å…‰ç…§æ¡ä»¶"""
        # éšæœºgammaè°ƒæ•´
        gamma = random.uniform(*self.gamma_range)
        return transforms.functional.adjust_gamma(image, gamma)
    
    def _shape_preserving_color_augmentation(self, image: Image.Image) -> Image.Image:
        """ä¿æŒå½¢çŠ¶ä¸å˜çš„é¢œè‰²å¢å¼º"""
        # è½¬æ¢ä¸ºHSVç©ºé—´
        img_array = np.array(image)
        hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV).astype(np.float32)
        
        # åªè°ƒæ•´è‰²è°ƒå’Œé¥±å’Œåº¦ï¼Œä¿æŒäº®åº¦ï¼ˆå½¢çŠ¶ä¿¡æ¯ï¼‰
        hsv[:, :, 0] = hsv[:, :, 0] * random.uniform(0.8, 1.2)  # è‰²è°ƒ
        hsv[:, :, 1] = hsv[:, :, 1] * random.uniform(0.7, 1.3)  # é¥±å’Œåº¦
        
        # ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        hsv[:, :, 0] = np.clip(hsv[:, :, 0], 0, 179)  # OpenCV HSV: H[0,179]
        hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)
        hsv[:, :, 2] = np.clip(hsv[:, :, 2], 0, 255)
        
        # è½¬æ¢å›RGB
        hsv = hsv.astype(np.uint8)
        augmented = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
        
        return Image.fromarray(augmented)
    
    def _adaptive_color_augmentation(self, image: Image.Image, target: int = None) -> Image.Image:
        """è‡ªé€‚åº”é¢œè‰²å¢å¼º - æ ¹æ®ç±»åˆ«è°ƒæ•´å¢å¼ºç­–ç•¥"""
        if target is None:
            return image
        
        # æ ¹æ®ç±»åˆ«é€‰æ‹©ä¸åŒçš„å¢å¼ºç­–ç•¥
        if target in [0, 1, 2, 3, 4, 5, 7, 8]:  # é€Ÿåº¦é™åˆ¶æ ‡å¿—
            # çº¢è‰²è¾¹æ¡†æ ‡å¿—ï¼Œå¢å¼ºçº¢è‰²é€šé“
            return self._enhance_red_channel(image)
        elif target in [14, 17]:  # åœæ­¢å’Œç¦æ­¢æ ‡å¿—
            # çº¢è‰²å¡«å……æ ‡å¿—ï¼Œå¢å¼ºçº¢è‰²å¯¹æ¯”åº¦
            return self._enhance_red_contrast(image)
        elif target in [33, 34, 35, 36, 37, 38, 39, 40]:  # è“è‰²æ–¹å‘æ ‡å¿—
            # è“è‰²æ ‡å¿—ï¼Œå¢å¼ºè“è‰²é€šé“
            return self._enhance_blue_channel(image)
        else:
            # å…¶ä»–æ ‡å¿—ï¼Œé€šç”¨å¢å¼º
            return self._general_color_enhancement(image)
    
    def _enhance_red_channel(self, image: Image.Image) -> Image.Image:
        """å¢å¼ºçº¢è‰²é€šé“"""
        img_array = np.array(image).astype(np.float32)
        
        # å¢å¼ºçº¢è‰²é€šé“
        img_array[:, :, 0] = np.clip(img_array[:, :, 0] * 1.2, 0, 255)
        
        # è½»å¾®æŠ‘åˆ¶å…¶ä»–é€šé“
        img_array[:, :, 1] = np.clip(img_array[:, :, 1] * 0.9, 0, 255)
        img_array[:, :, 2] = np.clip(img_array[:, :, 2] * 0.9, 0, 255)
        
        return Image.fromarray(img_array.astype(np.uint8))
    
    def _enhance_red_contrast(self, image: Image.Image) -> Image.Image:
        """å¢å¼ºçº¢è‰²å¯¹æ¯”åº¦"""
        img_array = np.array(image).astype(np.float32)
        
        # å¢å¼ºçº¢è‰²å¯¹æ¯”åº¦
        red_channel = img_array[:, :, 0]
        red_mean = np.mean(red_channel)
        red_std = np.std(red_channel)
        
        # æ ‡å‡†åŒ–å¹¶å¢å¼ºå¯¹æ¯”åº¦
        red_normalized = (red_channel - red_mean) / (red_std + 1e-8)
        red_enhanced = red_normalized * 1.5 + red_mean
        
        img_array[:, :, 0] = np.clip(red_enhanced, 0, 255)
        
        return Image.fromarray(img_array.astype(np.uint8))
    
    def _enhance_blue_channel(self, image: Image.Image) -> Image.Image:
        """å¢å¼ºè“è‰²é€šé“"""
        img_array = np.array(image).astype(np.float32)
        
        # å¢å¼ºè“è‰²é€šé“
        img_array[:, :, 2] = np.clip(img_array[:, :, 2] * 1.2, 0, 255)
        
        # è½»å¾®æŠ‘åˆ¶å…¶ä»–é€šé“
        img_array[:, :, 0] = np.clip(img_array[:, :, 0] * 0.9, 0, 255)
        img_array[:, :, 1] = np.clip(img_array[:, :, 1] * 0.9, 0, 255)
        
        return Image.fromarray(img_array.astype(np.uint8))
    
    def _general_color_enhancement(self, image: Image.Image) -> Image.Image:
        """é€šç”¨é¢œè‰²å¢å¼º"""
        img_array = np.array(image).astype(np.float32)
        
        # å¢å¼ºæ•´ä½“é¥±å’Œåº¦
        hsv = cv2.cvtColor(img_array.astype(np.uint8), cv2.COLOR_RGB2HSV).astype(np.float32)
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * 1.1, 0, 255)
        
        enhanced = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
        return Image.fromarray(enhanced)

class ColorInvarianceAugmentor:
    """é¢œè‰²ä¸å˜æ€§å¢å¼ºå™¨ - ç”Ÿæˆé¢œè‰²ä¸å˜çš„ç‰¹å¾"""
    
    def __init__(self):
        self.lighting_variations = [
            (0.8, 1.2),  # äº®åº¦å˜åŒ–
            (0.7, 1.3),  # å¯¹æ¯”åº¦å˜åŒ–
            (0.6, 1.4),  # é¥±å’Œåº¦å˜åŒ–
        ]
    
    def __call__(self, image: Image.Image) -> Image.Image:
        """åº”ç”¨é¢œè‰²ä¸å˜æ€§å¢å¼º"""
        # éšæœºé€‰æ‹©å…‰ç…§å˜åŒ–
        variation = random.choice(self.lighting_variations)
        
        # åº”ç”¨å˜åŒ–
        if random.random() < 0.5:
            # äº®åº¦å˜åŒ–
            factor = random.uniform(*variation)
            return transforms.functional.adjust_brightness(image, factor)
        else:
            # å¯¹æ¯”åº¦å˜åŒ–
            factor = random.uniform(*variation)
            return transforms.functional.adjust_contrast(image, factor)

class SemanticPreserveAugmentor:
    """è¯­ä¹‰ä¿æŒå¢å¼ºå™¨ - ä¿æŒé¢œè‰²è¯­ä¹‰çš„åŒæ—¶å¢å¼ºé²æ£’æ€§"""
    
    def __init__(self):
        # GTSRBé¢œè‰²è¯­ä¹‰æ˜ å°„
        self.color_semantic_map = {
            'red': [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42],
            'blue': [33, 34, 35, 36, 37, 38, 39, 40],
            'yellow': [12],
            'white': list(range(43)),  # æ‰€æœ‰ç±»åˆ«éƒ½æœ‰ç™½è‰²
            'black': [6, 9, 10, 11, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 41, 42]
        }
    
    def __call__(self, image: Image.Image, target: int) -> Image.Image:
        """åº”ç”¨è¯­ä¹‰ä¿æŒå¢å¼º"""
        # ç¡®å®šç›®æ ‡ç±»åˆ«çš„ä¸»è¦é¢œè‰²
        primary_colors = self._get_primary_colors(target)
        
        # æ ¹æ®ä¸»è¦é¢œè‰²è¿›è¡Œå¢å¼º
        if 'red' in primary_colors:
            return self._preserve_red_semantics(image)
        elif 'blue' in primary_colors:
            return self._preserve_blue_semantics(image)
        elif 'yellow' in primary_colors:
            return self._preserve_yellow_semantics(image)
        else:
            return self._preserve_general_semantics(image)
    
    def _get_primary_colors(self, target: int) -> list:
        """è·å–ç›®æ ‡ç±»åˆ«çš„ä¸»è¦é¢œè‰²"""
        primary_colors = []
        for color, classes in self.color_semantic_map.items():
            if target in classes:
                primary_colors.append(color)
        return primary_colors
    
    def _preserve_red_semantics(self, image: Image.Image) -> Image.Image:
        """ä¿æŒçº¢è‰²è¯­ä¹‰"""
        img_array = np.array(image).astype(np.float32)
        
        # ä¿æŒçº¢è‰²é€šé“çš„ç›¸å¯¹å¼ºåº¦
        red_ratio = img_array[:, :, 0] / (img_array[:, :, 1] + img_array[:, :, 2] + 1e-8)
        
        # å¢å¼ºçº¢è‰²è¯­ä¹‰
        red_enhanced = np.clip(red_ratio * 1.1, 0, 1)
        
        # åº”ç”¨å¢å¼º
        img_array[:, :, 0] = np.clip(img_array[:, :, 0] * red_enhanced, 0, 255)
        
        return Image.fromarray(img_array.astype(np.uint8))
    
    def _preserve_blue_semantics(self, image: Image.Image) -> Image.Image:
        """ä¿æŒè“è‰²è¯­ä¹‰"""
        img_array = np.array(image).astype(np.float32)
        
        # ä¿æŒè“è‰²é€šé“çš„ç›¸å¯¹å¼ºåº¦
        blue_ratio = img_array[:, :, 2] / (img_array[:, :, 0] + img_array[:, :, 1] + 1e-8)
        
        # å¢å¼ºè“è‰²è¯­ä¹‰
        blue_enhanced = np.clip(blue_ratio * 1.1, 0, 1)
        
        # åº”ç”¨å¢å¼º
        img_array[:, :, 2] = np.clip(img_array[:, :, 2] * blue_enhanced, 0, 255)
        
        return Image.fromarray(img_array.astype(np.uint8))
    
    def _preserve_yellow_semantics(self, image: Image.Image) -> Image.Image:
        """ä¿æŒé»„è‰²è¯­ä¹‰"""
        img_array = np.array(image).astype(np.float32)
        
        # é»„è‰² = çº¢è‰² + ç»¿è‰²
        yellow_intensity = (img_array[:, :, 0] + img_array[:, :, 1]) / 2
        
        # å¢å¼ºé»„è‰²è¯­ä¹‰
        yellow_enhanced = np.clip(yellow_intensity * 1.1, 0, 255)
        
        # åº”ç”¨å¢å¼º
        img_array[:, :, 0] = np.clip(img_array[:, :, 0] * 1.05, 0, 255)
        img_array[:, :, 1] = np.clip(img_array[:, :, 1] * 1.05, 0, 255)
        
        return Image.fromarray(img_array.astype(np.uint8))
    
    def _preserve_general_semantics(self, image: Image.Image) -> Image.Image:
        """ä¿æŒé€šç”¨è¯­ä¹‰"""
        # è½»å¾®çš„é¢œè‰²å¢å¼ºï¼Œä¿æŒæ•´ä½“è¯­ä¹‰
        return transforms.functional.adjust_saturation(image, 1.1)

class AdversarialColorAugmentor:
    """å¯¹æŠ—æ€§é¢œè‰²å¢å¼ºå™¨ - ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢œè‰²å˜åŒ–"""
    
    def __init__(self):
        self.adversarial_strength = 0.3
    
    def __call__(self, image: Image.Image) -> Image.Image:
        """åº”ç”¨å¯¹æŠ—æ€§é¢œè‰²å¢å¼º"""
        img_array = np.array(image).astype(np.float32)
        
        # éšæœºé€‰æ‹©å¯¹æŠ—æ€§ç­–ç•¥
        strategy = random.choice(['color_shift', 'channel_attack', 'semantic_confusion'])
        
        if strategy == 'color_shift':
            return self._color_shift_attack(img_array)
        elif strategy == 'channel_attack':
            return self._channel_attack(img_array)
        else:
            return self._semantic_confusion_attack(img_array)
    
    def _color_shift_attack(self, img_array: np.ndarray) -> Image.Image:
        """é¢œè‰²åç§»æ”»å‡»"""
        # éšæœºé¢œè‰²åç§»
        shift = np.random.uniform(-30, 30, 3)
        
        for i in range(3):
            img_array[:, :, i] = np.clip(img_array[:, :, i] + shift[i], 0, 255)
        
        return Image.fromarray(img_array.astype(np.uint8))
    
    def _channel_attack(self, img_array: np.ndarray) -> Image.Image:
        """é€šé“æ”»å‡»"""
        # éšæœºäº¤æ¢æˆ–æŠ‘åˆ¶é€šé“
        channels = [0, 1, 2]
        random.shuffle(channels)
        
        # è½»å¾®æŠ‘åˆ¶æŸäº›é€šé“
        for i, channel in enumerate(channels):
            if random.random() < 0.5:
                img_array[:, :, channel] = img_array[:, :, channel] * 0.8
        
        return Image.fromarray(img_array.astype(np.uint8))
    
    def _semantic_confusion_attack(self, img_array: np.ndarray) -> Image.Image:
        """è¯­ä¹‰æ··æ·†æ”»å‡»"""
        # è½¬æ¢ä¸ºHSVå¹¶æ·»åŠ å™ªå£°
        hsv = cv2.cvtColor(img_array.astype(np.uint8), cv2.COLOR_RGB2HSV).astype(np.float32)
        
        # åœ¨è‰²è°ƒé€šé“æ·»åŠ å™ªå£°
        noise = np.random.normal(0, 10, hsv.shape[:2])
        hsv[:, :, 0] = np.clip(hsv[:, :, 0] + noise, 0, 179)
        
        # è½¬æ¢å›RGB
        confused = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)
        return Image.fromarray(confused)

class ColorConsistencyLoss(nn.Module):
    """é¢œè‰²ä¸€è‡´æ€§æŸå¤±"""
    
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, features_orig: torch.Tensor, features_aug: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—é¢œè‰²ä¸€è‡´æ€§æŸå¤±"""
        # å½’ä¸€åŒ–ç‰¹å¾
        import torch.nn.functional as F
        features_orig_norm = F.normalize(features_orig, dim=1)
        features_aug_norm = F.normalize(features_aug, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(features_orig_norm, features_aug_norm.t()) / self.temperature
        
        # å¯¹è§’çº¿å…ƒç´ åº”è¯¥æ˜¯æœ€å¤§çš„ï¼ˆåŒä¸€å›¾åƒçš„ä¸åŒå¢å¼ºç‰ˆæœ¬ï¼‰
        labels = torch.arange(features_orig.size(0), device=features_orig.device)
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        loss = F.cross_entropy(similarity_matrix, labels)
        
        return loss

class ColorRobustnessLoss(nn.Module):
    """é¢œè‰²é²æ£’æ€§æŸå¤±"""
    
    def __init__(self, lambda_consistency=0.1, lambda_invariance=0.05):
        super().__init__()
        self.lambda_consistency = lambda_consistency
        self.lambda_invariance = lambda_invariance
        self.consistency_loss = ColorConsistencyLoss()
    
    def forward(self, 
                features_orig: torch.Tensor, 
                features_aug: torch.Tensor,
                predictions_orig: torch.Tensor,
                predictions_aug: torch.Tensor) -> Dict[str, torch.Tensor]:
        """è®¡ç®—é¢œè‰²é²æ£’æ€§æŸå¤±"""
        
        # ä¸€è‡´æ€§æŸå¤±
        consistency_loss = self.consistency_loss(features_orig, features_aug)
        
        # é¢„æµ‹ä¸å˜æ€§æŸå¤±
        import torch.nn.functional as F
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        predictions_orig = predictions_orig.float()
        predictions_aug = predictions_aug.float()
        invariance_loss = F.mse_loss(predictions_orig, predictions_aug)
        
        # æ€»æŸå¤±
        total_loss = (
            self.lambda_consistency * consistency_loss +
            self.lambda_invariance * invariance_loss
        )
        
        return {
            'total_loss': total_loss,
            'consistency_loss': consistency_loss,
            'invariance_loss': invariance_loss
        }

def test_color_augmentation():
    """æµ‹è¯•é«˜çº§é¢œè‰²å¢å¼ºæ¨¡å—"""
    print("ğŸ¨ æµ‹è¯•é«˜çº§é¢œè‰²å¢å¼ºæ¨¡å—")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    batch_size = 4
    channels = 3
    height, width = 32, 32
    
    # åˆ›å»ºå½©è‰²æµ‹è¯•å›¾åƒ
    test_images = torch.randn(batch_size, channels, height, width)
    test_targets = torch.randint(0, 43, (batch_size,))
    
    print(f"æµ‹è¯•å›¾åƒå½¢çŠ¶: {test_images.shape}")
    print(f"æµ‹è¯•ç›®æ ‡å½¢çŠ¶: {test_targets.shape}")
    
    # åˆ›å»ºé«˜çº§é¢œè‰²å¢å¼ºå™¨
    advanced_color_aug = AdvancedColorSpecificAugmentation()
    
    # åº”ç”¨å¢å¼º
    augmented_images = advanced_color_aug(test_images, test_targets)
    
    print(f"å¢å¼ºåå›¾åƒå½¢çŠ¶: {augmented_images.shape}")
    
    # æµ‹è¯•é¢œè‰²ä¸å˜æ€§å¢å¼ºå™¨
    invariance_augmentor = ColorInvarianceAugmentor()
    test_image = torch.randn(channels, height, width)
    test_pil = transforms.ToPILImage()(test_image)
    invariance_augmented = invariance_augmentor(test_pil)
    print(f"é¢œè‰²ä¸å˜æ€§å¢å¼º: {type(invariance_augmented)}")
    
    # æµ‹è¯•è¯­ä¹‰ä¿æŒå¢å¼ºå™¨
    semantic_augmentor = SemanticPreserveAugmentor()
    semantic_augmented = semantic_augmentor(test_pil, 0)  # é€Ÿåº¦é™åˆ¶æ ‡å¿—
    print(f"è¯­ä¹‰ä¿æŒå¢å¼º: {type(semantic_augmented)}")
    
    # æµ‹è¯•å¯¹æŠ—æ€§é¢œè‰²å¢å¼ºå™¨
    adversarial_augmentor = AdversarialColorAugmentor()
    adversarial_augmented = adversarial_augmentor(test_pil)
    print(f"å¯¹æŠ—æ€§é¢œè‰²å¢å¼º: {type(adversarial_augmented)}")
    
    # æµ‹è¯•é¢œè‰²ä¸€è‡´æ€§æŸå¤±
    consistency_loss = ColorConsistencyLoss()
    features_orig = torch.randn(batch_size, 64)
    features_aug = torch.randn(batch_size, 64)
    
    loss = consistency_loss(features_orig, features_aug)
    print(f"é¢œè‰²ä¸€è‡´æ€§æŸå¤±: {loss.item():.4f}")
    
    # æµ‹è¯•é¢œè‰²é²æ£’æ€§æŸå¤±
    robustness_loss = ColorRobustnessLoss()
    predictions_orig = torch.randn(batch_size, 43)
    predictions_aug = torch.randn(batch_size, 43)
    
    loss_dict = robustness_loss(features_orig, features_aug, predictions_orig, predictions_aug)
    
    print(f"é¢œè‰²é²æ£’æ€§æŸå¤±:")
    for key, value in loss_dict.items():
        print(f"  {key}: {value.item():.4f}")
    
    print(f"\nâœ… é«˜çº§é¢œè‰²å¢å¼ºæ¨¡å—æµ‹è¯•å®Œæˆ")

if __name__ == '__main__':
    test_color_augmentation()
