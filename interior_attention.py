# interior_attention.py
# Interior-Onlyæ³¨æ„åŠ›æœºåˆ¶ - ä¸“æ³¨æ ‡å¿—å†…éƒ¨åŒºåŸŸ

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Tuple
import cv2

class InteriorAttention(nn.Module):
    """Interior-Onlyæ³¨æ„åŠ›æœºåˆ¶ - ä¸“æ³¨æ ‡å¿—å†…éƒ¨åŒºåŸŸï¼Œæ’é™¤è¾¹æ¡†å’ŒèƒŒæ™¯"""
    
    def __init__(self, input_channels=3, attention_dim=64):
        super().__init__()
        self.input_channels = input_channels
        self.attention_dim = attention_dim
        
        # è¾¹ç¼˜æ£€æµ‹å™¨ - ç”¨äºè¯†åˆ«è¾¹æ¡†
        self.edge_detector = nn.Sequential(
            nn.Conv2d(input_channels, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(8, 1, 1),
            nn.Sigmoid()
        )
        
        # å½¢çŠ¶æ£€æµ‹å™¨ - ç”¨äºè¯†åˆ«å½¢çŠ¶è¾¹ç•Œ
        self.shape_detector = nn.Sequential(
            nn.Conv2d(input_channels, 16, 5, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(8, 1, 1),
            nn.Sigmoid()
        )
        
        # å†…éƒ¨åŒºåŸŸé¢„æµ‹å™¨
        self.interior_predictor = nn.Sequential(
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(8, 1, 1),
            nn.Sigmoid()
        )
        
        # æ³¨æ„åŠ›èåˆç½‘ç»œ
        self.attention_fusion = nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1),  # 3ä¸ªæ³¨æ„åŠ›å›¾
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(8, 1, 1),
            nn.Sigmoid()
        )
        
        # å†…éƒ¨ç‰¹å¾æå–å™¨
        self.interior_feature_extractor = nn.Sequential(
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU()
        )
        
        # å½¢çŠ¶æŠ‘åˆ¶å™¨ - æŠ‘åˆ¶å½¢çŠ¶ä¿¡æ¯
        self.shape_suppressor = nn.Sequential(
            nn.Conv2d(8, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(8, 8, 1),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1,1]ï¼Œç”¨äºæŠ‘åˆ¶
        )
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - ç”Ÿæˆå†…éƒ¨æ³¨æ„åŠ›æ©ç å’Œç‰¹å¾"""
        batch_size = x.size(0)
        
        # 1. è¾¹ç¼˜æ£€æµ‹ - è¯†åˆ«è¾¹æ¡†
        edge_map = self.edge_detector(x)
        
        # 2. å½¢çŠ¶æ£€æµ‹ - è¯†åˆ«å½¢çŠ¶è¾¹ç•Œ
        shape_map = self.shape_detector(x)
        
        # 3. å†…éƒ¨åŒºåŸŸé¢„æµ‹ - é¢„æµ‹çœŸæ­£çš„å†…éƒ¨åŒºåŸŸ
        interior_map = self.interior_predictor(x)
        
        # 4. æ³¨æ„åŠ›èåˆ - ç»“åˆä¸‰ç§æ³¨æ„åŠ›å›¾
        attention_inputs = torch.cat([edge_map, shape_map, interior_map], dim=1)
        fused_attention = self.attention_fusion(attention_inputs)
        
        # 5. ç”Ÿæˆå†…éƒ¨æ³¨æ„åŠ›æ©ç  - æ’é™¤è¾¹æ¡†å’Œå½¢çŠ¶è¾¹ç•Œ
        # è¾¹ç¼˜å’Œå½¢çŠ¶åŒºåŸŸåº”è¯¥è¢«æŠ‘åˆ¶
        edge_suppression = 1.0 - edge_map
        shape_suppression = 1.0 - shape_map
        
        # å†…éƒ¨æ³¨æ„åŠ›æ©ç  = å†…éƒ¨åŒºåŸŸ * è¾¹ç¼˜æŠ‘åˆ¶ * å½¢çŠ¶æŠ‘åˆ¶
        interior_attention_mask = interior_map * edge_suppression * shape_suppression
        
        # 6. æå–å†…éƒ¨ç‰¹å¾
        interior_features = self.interior_feature_extractor(x)
        
        # 7. å½¢çŠ¶æŠ‘åˆ¶ - æŠ‘åˆ¶å½¢çŠ¶ç›¸å…³çš„ç‰¹å¾
        shape_suppressed_features = self.shape_suppressor(interior_features)
        
        # 8. åº”ç”¨æ³¨æ„åŠ›æ©ç åˆ°ç‰¹å¾
        attended_features = interior_features * interior_attention_mask
        
        return attended_features, interior_attention_mask, edge_map, shape_map
    
    def compute_interior_attention_loss(self, attention_mask: torch.Tensor, 
                                      logits: torch.Tensor, 
                                      targets: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—å†…éƒ¨æ³¨æ„åŠ›æŸå¤±"""
        batch_size = attention_mask.size(0)
        
        # è®¡ç®—æ³¨æ„åŠ›å¼ºåº¦
        attention_strength = torch.mean(attention_mask, dim=[2, 3])  # [B, 1]
        
        # é¢œè‰²ç›¸å…³ç±»åˆ«éœ€è¦é«˜å†…éƒ¨æ³¨æ„åŠ›
        color_related_classes = [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]
        
        # åˆ›å»ºç›®æ ‡æ©ç 
        target_mask = torch.zeros_like(attention_strength)
        for i in range(batch_size):
            if targets[i].item() in color_related_classes:
                target_mask[i] = 0.8  # é¢œè‰²ç›¸å…³ç±»åˆ«éœ€è¦é«˜æ³¨æ„åŠ›
            else:
                target_mask[i] = 0.3  # å…¶ä»–ç±»åˆ«éœ€è¦è¾ƒä½æ³¨æ„åŠ›
        
        # è®¡ç®—æ³¨æ„åŠ›æŸå¤±
        attention_loss = F.mse_loss(attention_strength, target_mask)
        
        return attention_loss
    
    def compute_shape_suppression_loss(self, features: torch.Tensor, 
                                     attention_mask: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—å½¢çŠ¶æŠ‘åˆ¶æŸå¤±"""
        # è®¡ç®—ç‰¹å¾çš„ç©ºé—´æ¢¯åº¦ï¼ˆå½¢çŠ¶ä¿¡æ¯ï¼‰
        grad_x = torch.abs(features[:, :, :, 1:] - features[:, :, :, :-1])
        grad_y = torch.abs(features[:, :, 1:, :] - features[:, :, :-1, :])
        
        # è°ƒæ•´æ¢¯åº¦å°ºå¯¸ä»¥åŒ¹é…
        grad_x_padded = F.pad(grad_x, (0, 1, 0, 0))  # åœ¨å®½åº¦ç»´åº¦å¡«å……
        grad_y_padded = F.pad(grad_y, (0, 0, 0, 1))  # åœ¨é«˜åº¦ç»´åº¦å¡«å……
        
        shape_info = grad_x_padded + grad_y_padded
        
        # åœ¨å†…éƒ¨åŒºåŸŸï¼Œå½¢çŠ¶ä¿¡æ¯åº”è¯¥è¢«æŠ‘åˆ¶
        interior_shape_info = shape_info * attention_mask
        
        # æƒ©ç½šé«˜å½¢çŠ¶ä¿¡æ¯
        suppression_loss = torch.mean(interior_shape_info)
        
        return suppression_loss

class AdaptiveInteriorAttention(nn.Module):
    """è‡ªé€‚åº”å†…éƒ¨æ³¨æ„åŠ› - æ ¹æ®ç±»åˆ«åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›ç­–ç•¥"""
    
    def __init__(self, input_channels=3, num_classes=43):
        super().__init__()
        self.input_channels = input_channels
        self.num_classes = num_classes
        
        # åŸºç¡€å†…éƒ¨æ³¨æ„åŠ›
        self.base_interior_attention = InteriorAttention(input_channels)
        
        # ç±»åˆ«ç‰¹å®šçš„æ³¨æ„åŠ›æƒé‡
        self.class_attention_weights = nn.Parameter(torch.ones(num_classes))
        
        # è‡ªé€‚åº”èåˆç½‘ç»œ
        self.adaptive_fusion = nn.Sequential(
            nn.Linear(num_classes + 1, 32),  # ç±»åˆ« + åŸºç¡€æ³¨æ„åŠ›å¼ºåº¦
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
        # ç±»åˆ«åµŒå…¥
        self.class_embedding = nn.Embedding(num_classes, 16)
        
        # ç±»åˆ«æ„ŸçŸ¥ç‰¹å¾æå–
        self.class_aware_extractor = nn.Sequential(
            nn.Conv2d(input_channels + 16, 32, 3, padding=1),  # å›¾åƒ + ç±»åˆ«åµŒå…¥
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 16, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU()
        )
    
    def forward(self, x: torch.Tensor, targets: torch.Tensor = None) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        batch_size = x.size(0)
        
        # åŸºç¡€å†…éƒ¨æ³¨æ„åŠ›
        interior_features, attention_mask, edge_map, shape_map = self.base_interior_attention(x)
        
        # è®¡ç®—åŸºç¡€æ³¨æ„åŠ›å¼ºåº¦
        base_attention_strength = torch.mean(attention_mask, dim=[2, 3])
        
        if targets is not None:
            # ç±»åˆ«ç‰¹å®šçš„æ³¨æ„åŠ›è°ƒæ•´
            class_weights = self.class_attention_weights[targets]  # [B]
            
            # åˆ›å»ºç±»åˆ«one-hotç¼–ç 
            class_onehot = F.one_hot(targets, num_classes=self.num_classes).float()  # [B, num_classes]
            
            # è‡ªé€‚åº”èåˆ
            fusion_input = torch.cat([class_onehot, base_attention_strength], dim=1)
            adaptive_weight = self.adaptive_fusion(fusion_input)  # [B, 1]
            
            # ç±»åˆ«åµŒå…¥
            class_embeddings = self.class_embedding(targets)  # [B, 16]
            
            # æ‰©å±•ç±»åˆ«åµŒå…¥åˆ°ç©ºé—´ç»´åº¦
            _, _, height, width = x.shape
            class_embeddings_spatial = class_embeddings.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, height, width)
            
            # ç±»åˆ«æ„ŸçŸ¥ç‰¹å¾æå–
            class_aware_input = torch.cat([x, class_embeddings_spatial], dim=1)
            class_aware_features = self.class_aware_extractor(class_aware_input)
            
            # è‡ªé€‚åº”æ³¨æ„åŠ›æ©ç 
            adaptive_attention_mask = attention_mask * adaptive_weight.unsqueeze(-1).unsqueeze(-1)
            
            return {
                'interior_features': interior_features,
                'class_aware_features': class_aware_features,
                'attention_mask': adaptive_attention_mask,
                'base_attention_mask': attention_mask,
                'edge_map': edge_map,
                'shape_map': shape_map,
                'adaptive_weight': adaptive_weight,
                'class_weights': class_weights,
                'base_attention_strength': base_attention_strength
            }
        else:
            return {
                'interior_features': interior_features,
                'attention_mask': attention_mask,
                'edge_map': edge_map,
                'shape_map': shape_map,
                'base_attention_strength': base_attention_strength
            }
    
    def compute_adaptive_attention_loss(self, outputs: Dict[str, torch.Tensor], 
                                      targets: torch.Tensor) -> Dict[str, torch.Tensor]:
        """è®¡ç®—è‡ªé€‚åº”æ³¨æ„åŠ›æŸå¤±"""
        # åŸºç¡€å†…éƒ¨æ³¨æ„åŠ›æŸå¤±
        base_attention_loss = self.base_interior_attention.compute_interior_attention_loss(
            outputs['attention_mask'], None, targets
        )
        
        # å½¢çŠ¶æŠ‘åˆ¶æŸå¤±
        shape_suppression_loss = self.base_interior_attention.compute_shape_suppression_loss(
            outputs['interior_features'], outputs['attention_mask']
        )
        
        # ç±»åˆ«ä¸€è‡´æ€§æŸå¤± - ç¡®ä¿åŒç±»åˆ«çš„æ³¨æ„åŠ›æ¨¡å¼ç›¸ä¼¼
        class_attention_strengths = outputs['base_attention_strength']
        class_consistency_loss = 0.0
        
        for i in range(len(targets)):
            for j in range(i + 1, len(targets)):
                if targets[i] == targets[j]:
                    # åŒç±»åˆ«çš„æ³¨æ„åŠ›å¼ºåº¦åº”è¯¥ç›¸ä¼¼
                    consistency = torch.abs(class_attention_strengths[i] - class_attention_strengths[j])
                    class_consistency_loss += consistency
        
        if len(targets) > 1:
            class_consistency_loss = class_consistency_loss / (len(targets) * (len(targets) - 1) / 2)
        
        # è‡ªé€‚åº”æƒé‡æ­£åˆ™åŒ–
        adaptive_weight_regularization = torch.mean(outputs['adaptive_weight'])
        
        return {
            'base_attention_loss': base_attention_loss,
            'shape_suppression_loss': shape_suppression_loss,
            'class_consistency_loss': class_consistency_loss,
            'adaptive_weight_regularization': adaptive_weight_regularization,
            'total_loss': base_attention_loss + 0.1 * shape_suppression_loss + 
                         0.05 * class_consistency_loss + 0.01 * adaptive_weight_regularization
        }

def test_interior_attention():
    """æµ‹è¯•Interior-Onlyæ³¨æ„åŠ›æœºåˆ¶"""
    print("ğŸ¯ æµ‹è¯•Interior-Onlyæ³¨æ„åŠ›æœºåˆ¶")
    print("="*60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    channels = 3
    height, width = 32, 32
    
    test_images = torch.randn(batch_size, channels, height, width).to(device)
    test_targets = torch.randint(0, 43, (batch_size,)).to(device)
    
    print(f"æµ‹è¯•å›¾åƒå½¢çŠ¶: {test_images.shape}")
    print(f"æµ‹è¯•ç›®æ ‡å½¢çŠ¶: {test_targets.shape}")
    
    # æµ‹è¯•åŸºç¡€å†…éƒ¨æ³¨æ„åŠ›
    print(f"\nğŸ¯ æµ‹è¯•åŸºç¡€å†…éƒ¨æ³¨æ„åŠ›:")
    interior_attention = InteriorAttention().to(device)
    interior_features, attention_mask, edge_map, shape_map = interior_attention(test_images)
    
    print(f"  å†…éƒ¨ç‰¹å¾å½¢çŠ¶: {interior_features.shape}")
    print(f"  æ³¨æ„åŠ›æ©ç å½¢çŠ¶: {attention_mask.shape}")
    print(f"  è¾¹ç¼˜å›¾å½¢çŠ¶: {edge_map.shape}")
    print(f"  å½¢çŠ¶å›¾å½¢çŠ¶: {shape_map.shape}")
    
    # æµ‹è¯•æ³¨æ„åŠ›æŸå¤±
    attention_loss = interior_attention.compute_interior_attention_loss(
        attention_mask, None, test_targets
    )
    print(f"  æ³¨æ„åŠ›æŸå¤±: {attention_loss.item():.4f}")
    
    # æµ‹è¯•å½¢çŠ¶æŠ‘åˆ¶æŸå¤±
    suppression_loss = interior_attention.compute_shape_suppression_loss(
        interior_features, attention_mask
    )
    print(f"  å½¢çŠ¶æŠ‘åˆ¶æŸå¤±: {suppression_loss.item():.4f}")
    
    # æµ‹è¯•è‡ªé€‚åº”å†…éƒ¨æ³¨æ„åŠ›
    print(f"\nğŸ¯ æµ‹è¯•è‡ªé€‚åº”å†…éƒ¨æ³¨æ„åŠ›:")
    adaptive_attention = AdaptiveInteriorAttention().to(device)
    adaptive_outputs = adaptive_attention(test_images, test_targets)
    
    print(f"  è‡ªé€‚åº”è¾“å‡º:")
    for key, value in adaptive_outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"    {key}: {value.shape}")
    
    # æµ‹è¯•è‡ªé€‚åº”æŸå¤±
    adaptive_losses = adaptive_attention.compute_adaptive_attention_loss(adaptive_outputs, test_targets)
    print(f"  è‡ªé€‚åº”æŸå¤±:")
    for key, value in adaptive_losses.items():
        if isinstance(value, torch.Tensor):
            print(f"    {key}: {value.item():.4f}")
        else:
            print(f"    {key}: {value:.4f}")
    
    print(f"\nâœ… Interior-Onlyæ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•å®Œæˆ")

if __name__ == '__main__':
    test_interior_attention()
