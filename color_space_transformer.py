# color_space_transformer.py
# é¢œè‰²ç©ºé—´è½¬æ¢æ¨¡å— - æä¾›é¢œè‰²æ„ŸçŸ¥ç‰¹å¾

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
from typing import Dict, Tuple
import math

class TrueColorSpaceConverter(nn.Module):
    """çœŸæ­£çš„é¢œè‰²ç©ºé—´è½¬æ¢å™¨ - åŸºäºæ•°å­¦å…¬å¼"""
    
    def __init__(self):
        super().__init__()
        
        # é¢„å®šä¹‰çš„è½¬æ¢çŸ©é˜µ - æ³¨å†Œä¸ºç¼“å†²åŒºï¼Œä¼šè‡ªåŠ¨ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        self.register_buffer('rgb_to_xyz_matrix', torch.tensor([
            [0.4124, 0.3576, 0.1805],
            [0.2126, 0.7152, 0.0722],
            [0.0193, 0.1192, 0.9505]
        ], dtype=torch.float32))
        
        self.register_buffer('xyz_to_lab_matrix', torch.tensor([
            [0.0, 116.0, 0.0],
            [500.0, -500.0, 0.0],
            [0.0, 200.0, -200.0]
        ], dtype=torch.float32))
    
    def rgb_to_hsv(self, rgb: torch.Tensor) -> torch.Tensor:
        """RGBåˆ°HSVçš„çœŸæ­£è½¬æ¢"""
        # å½’ä¸€åŒ–åˆ°[0,1]
        rgb_norm = torch.clamp(rgb, 0, 1)
        
        # è®¡ç®—æœ€å¤§å€¼å’Œæœ€å°å€¼
        max_rgb, _ = torch.max(rgb_norm, dim=1, keepdim=True)
        min_rgb, _ = torch.min(rgb_norm, dim=1, keepdim=True)
        diff = max_rgb - min_rgb
        
        # è®¡ç®—é¥±å’Œåº¦
        saturation = torch.where(max_rgb > 0, diff / max_rgb, torch.zeros_like(diff))
        
        # è®¡ç®—è‰²è°ƒ
        hue = torch.zeros_like(rgb_norm[:, :1, :, :])
        
        # Ré€šé“æœ€å¤§
        r_mask = (rgb_norm[:, 0:1, :, :] == max_rgb) & (diff > 0)
        hue[r_mask] = (60 * ((rgb_norm[:, 1:2, :, :] - rgb_norm[:, 2:3, :, :]) / diff))[r_mask] % 360
        
        # Gé€šé“æœ€å¤§
        g_mask = (rgb_norm[:, 1:2, :, :] == max_rgb) & (diff > 0)
        hue[g_mask] = (60 * ((rgb_norm[:, 2:3, :, :] - rgb_norm[:, 0:1, :, :]) / diff + 2))[g_mask] % 360
        
        # Bé€šé“æœ€å¤§
        b_mask = (rgb_norm[:, 2:3, :, :] == max_rgb) & (diff > 0)
        hue[b_mask] = (60 * ((rgb_norm[:, 0:1, :, :] - rgb_norm[:, 1:2, :, :]) / diff + 4))[b_mask] % 360
        
        # å½’ä¸€åŒ–è‰²è°ƒåˆ°[0,1]
        hue = hue / 360.0
        
        return torch.cat([hue, saturation, max_rgb], dim=1)
    
    def rgb_to_lab(self, rgb: torch.Tensor) -> torch.Tensor:
        """RGBåˆ°Labçš„çœŸæ­£è½¬æ¢"""
        # å½’ä¸€åŒ–åˆ°[0,1]
        rgb_norm = torch.clamp(rgb, 0, 1)
        
        # åº”ç”¨gammaæ ¡æ­£
        rgb_gamma = torch.where(rgb_norm > 0.04045, 
                               torch.pow((rgb_norm + 0.055) / 1.055, 2.4),
                               rgb_norm / 12.92)
        
        # RGBåˆ°XYZè½¬æ¢
        batch_size, _, height, width = rgb_gamma.shape
        rgb_flat = rgb_gamma.view(batch_size, 3, -1)
        
        # çŸ©é˜µä¹˜æ³•
        xyz_flat = torch.bmm(self.rgb_to_xyz_matrix.unsqueeze(0).expand(batch_size, -1, -1), rgb_flat)
        xyz = xyz_flat.view(batch_size, 3, height, width)
        
        # å½’ä¸€åŒ–XYZ
        xyz_norm = xyz / torch.tensor([0.9505, 1.0, 1.0890], device=xyz.device).view(1, 3, 1, 1)
        
        # XYZåˆ°Labè½¬æ¢
        xyz_flat = xyz_norm.view(batch_size, 3, -1)
        lab_flat = torch.bmm(self.xyz_to_lab_matrix.unsqueeze(0).expand(batch_size, -1, -1), xyz_flat)
        lab = lab_flat.view(batch_size, 3, height, width)
        
        # åº”ç”¨éçº¿æ€§å˜æ¢
        lab[:, 0, :, :] = 116 * torch.pow(xyz_norm[:, 1, :, :], 1/3) - 16  # L
        lab[:, 1, :, :] = 500 * (torch.pow(xyz_norm[:, 0, :, :], 1/3) - torch.pow(xyz_norm[:, 1, :, :], 1/3))  # a
        lab[:, 2, :, :] = 200 * (torch.pow(xyz_norm[:, 1, :, :], 1/3) - torch.pow(xyz_norm[:, 2, :, :], 1/3))  # b
        
        return lab

class ColorInvarianceFeatures(nn.Module):
    """é¢œè‰²ä¸å˜æ€§ç‰¹å¾æå–å™¨"""
    
    def __init__(self, input_channels=3):
        super().__init__()
        
        # ç°åº¦ç‰¹å¾æå–
        self.grayscale_extractor = nn.Sequential(
            nn.Conv2d(input_channels, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU()
        )
        
        # è¾¹ç¼˜ç‰¹å¾æå–
        self.edge_extractor = nn.Sequential(
            nn.Conv2d(input_channels, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU()
        )
        
        # çº¹ç†ç‰¹å¾æå–
        self.texture_extractor = nn.Sequential(
            nn.Conv2d(input_channels, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 8, 3, padding=1),
            nn.ReLU()
        )
        
        # ç‰¹å¾èåˆ
        self.feature_fusion = nn.Sequential(
            nn.Conv2d(24, 16, 1),  # 8*3 = 24
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 8, 1),
            nn.ReLU()
        )
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # ç°åº¦ç‰¹å¾
        grayscale = self.grayscale_extractor(x)
        
        # è¾¹ç¼˜ç‰¹å¾
        edges = self.edge_extractor(x)
        
        # çº¹ç†ç‰¹å¾
        texture = self.texture_extractor(x)
        
        # èåˆç‰¹å¾
        combined = torch.cat([grayscale, edges, texture], dim=1)
        fused = self.feature_fusion(combined)
        
        return {
            'grayscale': grayscale,
            'edges': edges,
            'texture': texture,
            'fused': fused,
            'combined': combined
        }

class MultiScaleColorAnalysis(nn.Module):
    """å¤šå°ºåº¦é¢œè‰²åˆ†æ"""
    
    def __init__(self, input_channels=3):
        super().__init__()
        
        # ä¸åŒå°ºåº¦çš„å·ç§¯æ ¸
        self.scale1 = nn.Sequential(
            nn.Conv2d(input_channels, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU()
        )
        
        self.scale2 = nn.Sequential(
            nn.Conv2d(input_channels, 16, 5, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU()
        )
        
        self.scale3 = nn.Sequential(
            nn.Conv2d(input_channels, 16, 7, padding=3),
            nn.BatchNorm2d(16),
            nn.ReLU()
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.scale_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(48, 16, 1),  # 16*3 = 48
            nn.ReLU(),
            nn.Conv2d(16, 3, 1),  # 3ä¸ªå°ºåº¦
            nn.Softmax(dim=1)
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(48, 32, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 16, 1),
            nn.ReLU()
        )
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # å¤šå°ºåº¦ç‰¹å¾
        scale1_feat = self.scale1(x)
        scale2_feat = self.scale2(x)
        scale3_feat = self.scale3(x)
        
        # æ‹¼æ¥æ‰€æœ‰å°ºåº¦ç‰¹å¾
        all_scales = torch.cat([scale1_feat, scale2_feat, scale3_feat], dim=1)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = self.scale_attention(all_scales)
        
        # åŠ æƒèåˆ
        weighted_features = []
        for i in range(3):
            start_idx = i * 16
            end_idx = (i + 1) * 16
            weighted = all_scales[:, start_idx:end_idx, :, :] * attention_weights[:, i:i+1, :, :]
            weighted_features.append(weighted)
        
        weighted_combined = torch.cat(weighted_features, dim=1)
        
        # æœ€ç»ˆèåˆ
        fused = self.fusion(weighted_combined)
        
        return {
            'scale1': scale1_feat,
            'scale2': scale2_feat,
            'scale3': scale3_feat,
            'attention_weights': attention_weights,
            'weighted_features': weighted_combined,
            'fused': fused
        }

class ColorSemanticEncoder(nn.Module):
    """é¢œè‰²è¯­ä¹‰ç¼–ç å™¨ - å­¦ä¹ é¢œè‰²è¯­ä¹‰è¡¨ç¤º"""
    
    def __init__(self, input_channels=3, num_colors=7):
        super().__init__()
        self.num_colors = num_colors
        
        # é¢œè‰²è¯­ä¹‰ç‰¹å¾æå–
        self.semantic_extractor = nn.Sequential(
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU()
        )
        
        # é¢œè‰²åˆ†ç±»å™¨
        self.color_classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, num_colors),
            nn.Softmax(dim=1)
        )
        
        # è¯­ä¹‰åµŒå…¥
        self.semantic_embedding = nn.Embedding(num_colors, 16)
        
        # è¯­ä¹‰èåˆ
        self.semantic_fusion = nn.Sequential(
            nn.Conv2d(32 + 16, 32, 1),  # ç‰¹å¾ + è¯­ä¹‰åµŒå…¥
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 16, 1),
            nn.ReLU()
        )
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # è¯­ä¹‰ç‰¹å¾æå–
        semantic_features = self.semantic_extractor(x)
        
        # é¢œè‰²åˆ†ç±»
        color_probs = self.color_classifier(semantic_features)
        color_indices = torch.argmax(color_probs, dim=1)
        
        # è¯­ä¹‰åµŒå…¥
        semantic_embeddings = self.semantic_embedding(color_indices)  # [B, 16]
        
        # æ‰©å±•è¯­ä¹‰åµŒå…¥åˆ°ç©ºé—´ç»´åº¦
        batch_size, _, height, width = semantic_features.shape
        semantic_embeddings_spatial = semantic_embeddings.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, height, width)
        
        # èåˆç‰¹å¾å’Œè¯­ä¹‰
        combined = torch.cat([semantic_features, semantic_embeddings_spatial], dim=1)
        fused = self.semantic_fusion(combined)
        
        return {
            'semantic_features': semantic_features,
            'color_probs': color_probs,
            'color_indices': color_indices,
            'semantic_embeddings': semantic_embeddings,
            'fused': fused
        }

class ColorSpaceTransformer(nn.Module):
    """å¢å¼ºçš„é¢œè‰²ç©ºé—´è½¬æ¢æ¨¡å— - æ•´åˆæ‰€æœ‰é¢œè‰²æ„ŸçŸ¥åŠŸèƒ½"""
    
    def __init__(self, input_channels=3):
        super().__init__()
        self.input_channels = input_channels
        
        # çœŸæ­£çš„é¢œè‰²ç©ºé—´è½¬æ¢å™¨
        self.true_converter = TrueColorSpaceConverter()
        
        # é¢œè‰²ä¸å˜æ€§ç‰¹å¾
        self.invariance_features = ColorInvarianceFeatures(input_channels)
        
        # å¤šå°ºåº¦é¢œè‰²åˆ†æ
        self.multi_scale_analysis = MultiScaleColorAnalysis(input_channels)
        
        # é¢œè‰²è¯­ä¹‰ç¼–ç å™¨
        self.semantic_encoder = ColorSemanticEncoder(input_channels)
        
        # é¢œè‰²é€šé“æ³¨æ„åŠ›
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(input_channels, 8, 1),
            nn.ReLU(),
            nn.Conv2d(8, input_channels, 1),
            nn.Sigmoid()
        )
        
        # å…¨å±€é¢œè‰²ç©ºé—´èåˆ
        # è®¡ç®—æ€»é€šé“æ•°: RGB(3) + HSV(3) + Lab(3) + å¯¹æ‰‹è‰²(3) + ä¸å˜æ€§(8) + å¤šå°ºåº¦(16) + è¯­ä¹‰(16) = 52
        total_channels = input_channels * 4 + 8 + 16 + 16
        self.global_fusion = nn.Sequential(
            nn.Conv2d(total_channels, 64, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 32, 1),
            nn.ReLU(),
            nn.Conv2d(32, input_channels, 1)
        )
        
        # é¢œè‰²ä¸€è‡´æ€§æŸå¤±æƒé‡
        self.color_consistency_weight = nn.Parameter(torch.tensor(0.1))
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­ - ç”Ÿæˆå¢å¼ºçš„é¢œè‰²æ„ŸçŸ¥ç‰¹å¾"""
        batch_size = x.size(0)
        
        # åŸå§‹RGBç‰¹å¾
        rgb_features = x
        
        # çœŸæ­£çš„é¢œè‰²ç©ºé—´è½¬æ¢
        hsv_features = self.true_converter.rgb_to_hsv(x)
        lab_features = self.true_converter.rgb_to_lab(x)
        
        # å¯¹æ‰‹è‰²ç©ºé—´ (R-G, R+G-2B, R+G+B)
        opponent_features = torch.stack([
            x[:, 0, :, :] - x[:, 1, :, :],  # R-G
            x[:, 0, :, :] + x[:, 1, :, :] - 2 * x[:, 2, :, :],  # R+G-2B
            x[:, 0, :, :] + x[:, 1, :, :] + x[:, 2, :, :]  # R+G+B
        ], dim=1)
        
        # é¢œè‰²ä¸å˜æ€§ç‰¹å¾
        invariance_outputs = self.invariance_features(x)
        
        # å¤šå°ºåº¦é¢œè‰²åˆ†æ
        multi_scale_outputs = self.multi_scale_analysis(x)
        
        # é¢œè‰²è¯­ä¹‰ç¼–ç 
        semantic_outputs = self.semantic_encoder(x)
        
        # é¢œè‰²é€šé“æ³¨æ„åŠ›
        channel_weights = self.channel_attention(x)
        attended_rgb = rgb_features * channel_weights
        
        # å…¨å±€é¢œè‰²ç©ºé—´èåˆ
        all_color_spaces = torch.cat([
            attended_rgb,           # RGB
            hsv_features,           # HSV
            lab_features,           # Lab
            opponent_features,      # å¯¹æ‰‹è‰²
            invariance_outputs['fused'],      # ä¸å˜æ€§ç‰¹å¾
            multi_scale_outputs['fused'],     # å¤šå°ºåº¦ç‰¹å¾
            semantic_outputs['fused']         # è¯­ä¹‰ç‰¹å¾
        ], dim=1)
        
        fused_features = self.global_fusion(all_color_spaces)
        
        return {
            'rgb': attended_rgb,
            'hsv': hsv_features,
            'lab': lab_features,
            'opponent': opponent_features,
            'invariance': invariance_outputs,
            'multi_scale': multi_scale_outputs,
            'semantic': semantic_outputs,
            'channel_weights': channel_weights,
            'fused': fused_features,
            'all_spaces': all_color_spaces
        }
    
    def compute_color_consistency_loss(self, outputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """è®¡ç®—é¢œè‰²ä¸€è‡´æ€§æŸå¤±"""
        # è®¡ç®—ä¸åŒé¢œè‰²ç©ºé—´ä¹‹é—´çš„ç›¸å…³æ€§
        hsv_flat = outputs['hsv'].view(outputs['hsv'].size(0), -1)
        lab_flat = outputs['lab'].view(outputs['lab'].size(0), -1)
        
        # å½’ä¸€åŒ–
        hsv_norm = F.normalize(hsv_flat, dim=1)
        lab_norm = F.normalize(lab_flat, dim=1)
        
        # è®¡ç®—ç›¸å…³æ€§
        correlation = torch.mm(hsv_norm, lab_norm.t())
        
        # æƒ©ç½šé«˜ç›¸å…³æ€§ï¼ˆé¼“åŠ±å¤šæ ·æ€§ï¼‰
        consistency_loss = torch.mean(torch.abs(correlation))
        
        return consistency_loss * self.color_consistency_weight

def test_color_space_transformer():
    """æµ‹è¯•å¢å¼ºçš„é¢œè‰²ç©ºé—´è½¬æ¢æ¨¡å—"""
    print("ğŸ¨ æµ‹è¯•å¢å¼ºçš„é¢œè‰²ç©ºé—´è½¬æ¢æ¨¡å—")
    print("="*60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    channels = 3
    height, width = 32, 32
    
    test_images = torch.randn(batch_size, channels, height, width).to(device)
    print(f"æµ‹è¯•å›¾åƒå½¢çŠ¶: {test_images.shape}")
    
    # æµ‹è¯•å¢å¼ºçš„é¢œè‰²ç©ºé—´è½¬æ¢å™¨
    color_transformer = ColorSpaceTransformer().to(device)
    color_outputs = color_transformer(test_images)
    
    print(f"\nğŸ“Š å¢å¼ºé¢œè‰²ç©ºé—´è½¬æ¢å™¨è¾“å‡º:")
    for key, value in color_outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")
        elif isinstance(value, dict):
            print(f"  {key}: {type(value)}")
            for sub_key, sub_value in value.items():
                if isinstance(sub_value, torch.Tensor):
                    print(f"    {sub_key}: {sub_value.shape}")
    
    # æµ‹è¯•é¢œè‰²ä¸€è‡´æ€§æŸå¤±
    consistency_loss = color_transformer.compute_color_consistency_loss(color_outputs)
    print(f"\nğŸ¯ é¢œè‰²ä¸€è‡´æ€§æŸå¤±: {consistency_loss.item():.4f}")
    
    print(f"\nâœ… å¢å¼ºé¢œè‰²ç©ºé—´è½¬æ¢æ¨¡å—æµ‹è¯•å®Œæˆ")

if __name__ == '__main__':
    test_color_space_transformer()
