# parameter_analysis.py
# é¢œè‰²å¤´å‚æ•°é‡è¯¦ç»†åˆ†æ

import torch
import torch.nn as nn
import sys
import os
from collections import defaultdict

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from complete_enhanced_color_head import CompleteEnhancedColorHead
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥æ¨¡å—: {e}")
    exit(1)

def analyze_parameters():
    """åˆ†æé¢œè‰²å¤´å‚æ•°é‡"""
    print("ğŸ¨ é¢œè‰²å¤´å‚æ•°é‡è¯¦ç»†åˆ†æ")
    print("="*60)
    
    # åˆ›å»ºé¢œè‰²å¤´
    color_head = CompleteEnhancedColorHead(input_dim=64, num_classes=43, color_dim=7)
    
    # æ€»ä½“ç»Ÿè®¡
    total_params = sum(p.numel() for p in color_head.parameters())
    trainable_params = sum(p.numel() for p in color_head.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024:.2f} KB (float32)")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (float32)")
    
    # æŒ‰æ¨¡å—åˆ†ç»„ç»Ÿè®¡
    module_params = defaultdict(int)
    module_details = defaultdict(list)
    
    for name, param in color_head.named_parameters():
        # æå–æ¨¡å—å
        module_name = name.split('.')[0] if '.' in name else name
        module_params[module_name] += param.numel()
        module_details[module_name].append((name, param.numel(), param.shape))
    
    print(f"\nğŸ“‹ å„æ¨¡å—å‚æ•°é‡:")
    print("-" * 50)
    
    for module_name in sorted(module_params.keys()):
        params_count = module_params[module_name]
        percentage = params_count / total_params * 100
        print(f"{module_name:25s}: {params_count:8,} å‚æ•° ({percentage:5.1f}%)")
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        for param_name, param_count, param_shape in module_details[module_name]:
            print(f"  â””â”€ {param_name:30s}: {param_count:6,} å‚æ•° {param_shape}")
    
    # æŒ‰å±‚ç±»å‹ç»Ÿè®¡
    layer_types = defaultdict(int)
    for name, module in color_head.named_modules():
        if isinstance(module, nn.Linear):
            layer_types['Linear'] += sum(p.numel() for p in module.parameters())
        elif isinstance(module, nn.Conv2d):
            layer_types['Conv2d'] += sum(p.numel() for p in module.parameters())
        elif isinstance(module, nn.BatchNorm1d):
            layer_types['BatchNorm1d'] += sum(p.numel() for p in module.parameters())
        elif isinstance(module, nn.BatchNorm2d):
            layer_types['BatchNorm2d'] += sum(p.numel() for p in module.parameters())
        elif isinstance(module, nn.Parameter):
            layer_types['Parameter'] += module.numel()
    
    print(f"\nğŸ”§ æŒ‰å±‚ç±»å‹ç»Ÿè®¡:")
    print("-" * 30)
    for layer_type, count in sorted(layer_types.items()):
        percentage = count / total_params * 100
        print(f"{layer_type:15s}: {count:8,} å‚æ•° ({percentage:5.1f}%)")
    
    # è®¡ç®—FLOPs (ç®€åŒ–ä¼°ç®—)
    print(f"\nâš¡ è®¡ç®—å¤æ‚åº¦ä¼°ç®—:")
    print("-" * 30)
    
    # å‡è®¾è¾“å…¥batch_size=32, ç‰¹å¾ç»´åº¦=64
    batch_size = 32
    input_dim = 64
    
    # ä¸»è¦è®¡ç®—è·¯å¾„
    flops = 0
    
    # ShapeDecorrelator
    flops += batch_size * (64 * 32 + 32 * 16) * 2  # ä¸¤ä¸ªextractor
    flops += batch_size * 32 * 16  # decorr_projection
    
    # Color Feature Extractor
    flops += batch_size * (64 * 64 + 64 * 32 + 32 * 16)
    
    # Multimodal Fusion
    flops += batch_size * (128 * 64 + 64 * 32 + 32 * 16)
    
    # Color Semantic Head
    flops += batch_size * (16 * 8 + 8 * 43)
    
    # Color Detector
    flops += batch_size * (16 * 8 + 8 * 7)
    
    print(f"  ä¼°ç®—FLOPs: {flops:,}")
    print(f"  æ¯æ ·æœ¬FLOPs: {flops // batch_size:,}")
    
    # å†…å­˜ä½¿ç”¨ä¼°ç®—
    print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨ä¼°ç®—:")
    print("-" * 30)
    
    # æ¨¡å‹å‚æ•°å†…å­˜
    param_memory = total_params * 4  # float32 = 4 bytes
    
    # æ¿€æ´»å†…å­˜ (ç®€åŒ–ä¼°ç®—)
    activation_memory = batch_size * (64 + 16 + 16 + 43 + 7) * 4  # ä¸»è¦æ¿€æ´»
    
    # æ¢¯åº¦å†…å­˜
    gradient_memory = total_params * 4
    
    total_memory = param_memory + activation_memory + gradient_memory
    
    print(f"  å‚æ•°å†…å­˜: {param_memory / 1024 / 1024:.2f} MB")
    print(f"  æ¿€æ´»å†…å­˜: {activation_memory / 1024 / 1024:.2f} MB")
    print(f"  æ¢¯åº¦å†…å­˜: {gradient_memory / 1024 / 1024:.2f} MB")
    print(f"  æ€»å†…å­˜: {total_memory / 1024 / 1024:.2f} MB")
    
    # ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”
    print(f"\nğŸ“ˆ å‚æ•°é‡å¯¹æ¯”:")
    print("-" * 30)
    
    comparisons = {
        "ResNet-18": 11_689_512,
        "ResNet-50": 25_557_032,
        "VGG-16": 138_357_544,
        "æˆ‘ä»¬çš„é¢œè‰²å¤´": total_params,
        "ç®€å•MLP (64â†’32â†’43)": 64*32 + 32*43 + 32 + 43,
        "CNN (3x3, 64é€šé“)": 3*3*3*64 + 64,
    }
    
    for model_name, params in comparisons.items():
        print(f"{model_name:20s}: {params:10,} å‚æ•°")
    
    return {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'module_params': dict(module_params),
        'layer_types': dict(layer_types)
    }

def compare_with_baseline():
    """ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”"""
    print(f"\nğŸ”„ ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”:")
    print("="*40)
    
    # åŸºçº¿æ¨¡å‹ - ç®€å•çš„é¢œè‰²å¤´
    class BaselineColorHead(nn.Module):
        def __init__(self, input_dim=64, num_classes=43):
            super().__init__()
            self.classifier = nn.Sequential(
                nn.Linear(input_dim, 32),
                nn.ReLU(),
                nn.Linear(32, num_classes)
            )
        
        def forward(self, x):
            return self.classifier(x)
    
    baseline = BaselineColorHead()
    baseline_params = sum(p.numel() for p in baseline.parameters())
    
    # æˆ‘ä»¬çš„å¢å¼ºé¢œè‰²å¤´
    enhanced = CompleteEnhancedColorHead(input_dim=64, num_classes=43)
    enhanced_params = sum(p.numel() for p in enhanced.parameters())
    
    print(f"åŸºçº¿é¢œè‰²å¤´å‚æ•°é‡: {baseline_params:,}")
    print(f"å¢å¼ºé¢œè‰²å¤´å‚æ•°é‡: {enhanced_params:,}")
    print(f"å‚æ•°é‡å¢åŠ : {enhanced_params / baseline_params:.1f}x")
    print(f"å‚æ•°é‡å¢åŠ : {enhanced_params - baseline_params:,} å‚æ•°")
    
    # åŠŸèƒ½å¯¹æ¯”
    print(f"\nğŸ¯ åŠŸèƒ½å¯¹æ¯”:")
    print(f"åŸºçº¿é¢œè‰²å¤´:")
    print(f"  - ç®€å•çº¿æ€§åˆ†ç±»")
    print(f"  - æ— å½’çº³åç½®")
    print(f"  - æ— å½¢çŠ¶å»ç›¸å…³")
    print(f"  - æ— ç¬¦å·çŸ¥è¯†é›†æˆ")
    
    print(f"\nå¢å¼ºé¢œè‰²å¤´:")
    print(f"  - å¤šæ¨¡æ€ç‰¹å¾èåˆ")
    print(f"  - å¼ºå½’çº³åç½®")
    print(f"  - å½¢çŠ¶å»ç›¸å…³æœºåˆ¶")
    print(f"  - ç¬¦å·çŸ¥è¯†é©±åŠ¨")
    print(f"  - è‡ªé€‚åº”æƒé‡å­¦ä¹ ")
    print(f"  - 6ç§æŸå¤±å‡½æ•°")

if __name__ == '__main__':
    results = analyze_parameters()
    compare_with_baseline()
    
    print(f"\nâœ… å‚æ•°é‡åˆ†æå®Œæˆ!")
    print(f"æˆ‘ä»¬çš„é¢œè‰²å¤´æ˜¯ä¸€ä¸ªè½»é‡çº§ä½†åŠŸèƒ½å¼ºå¤§çš„æ¨¡å—ï¼Œ")
    print(f"åœ¨ä¿æŒè¾ƒä½å‚æ•°é‡çš„åŒæ—¶å®ç°äº†å¼ºå½’çº³åç½®ã€‚")
