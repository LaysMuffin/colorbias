import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import os
import json
from collections import defaultdict
import argparse
import random

# å¯¼å…¥æ¨¡å‹
from improved_color_combination import (
    ImprovedColorCombinationHead, 
    BalancedColorCombinationDataset,
    IMPROVED_COLOR_COMBINATIONS
)

class RealGTSRBCAMGenerator:
    """çœŸå®GTSRBå›¾ç‰‡CAMç”Ÿæˆå™¨"""
    
    def __init__(self, model_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = ImprovedColorCombinationHead(num_classes=5).to(self.device)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.eval()
        
        # è·å–æœ€åä¸€ä¸ªå·ç§¯å±‚çš„ç‰¹å¾
        self.features = None
        self.gradients = None
        
        # æ³¨å†Œé’©å­
        self._register_hooks()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {model_path}")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _register_hooks(self):
        """æ³¨å†Œé’©å­æ¥è·å–ç‰¹å¾å’Œæ¢¯åº¦"""
        def forward_hook(module, input, output):
            self.features = output
        
        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0]
        
        # è·å–æœ€åä¸€ä¸ªå·ç§¯å±‚
        last_conv_layer = None
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Conv2d):
                last_conv_layer = module
        
        if last_conv_layer is not None:
            last_conv_layer.register_forward_hook(forward_hook)
            last_conv_layer.register_backward_hook(backward_hook)
            print(f"âœ… å·²æ³¨å†Œé’©å­åˆ°å·ç§¯å±‚: {last_conv_layer}")
        else:
            print("âŒ æœªæ‰¾åˆ°å·ç§¯å±‚")
    
    def generate_cam(self, image, target_class=None):
        """ç”ŸæˆCAM"""
        # å‡†å¤‡è¾“å…¥
        if isinstance(image, np.ndarray):
            # å¦‚æœæ˜¯HWCæ ¼å¼ï¼Œè½¬æ¢ä¸ºCHWæ ¼å¼
            if image.shape[-1] == 3:  # HWC -> CHW
                image = np.transpose(image, (2, 0, 1))
            image = torch.FloatTensor(image).unsqueeze(0)
        elif isinstance(image, torch.Tensor):
            # å¦‚æœæ˜¯HWCæ ¼å¼ï¼Œè½¬æ¢ä¸ºCHWæ ¼å¼
            if image.shape[-1] == 3:  # HWC -> CHW
                image = image.permute(2, 0, 1)
            image = image.unsqueeze(0)
        
        image = image.to(self.device)
        image.requires_grad = True
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(image)
        logits = outputs['combination_logits']
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ç±»åˆ«ï¼Œä½¿ç”¨é¢„æµ‹çš„ç±»åˆ«
        if target_class is None:
            target_class = logits.argmax(dim=1).item()
        
        # è®¡ç®—ç›®æ ‡ç±»åˆ«çš„å¾—åˆ†
        score = logits[0, target_class]
        
        # åå‘ä¼ æ’­
        self.model.zero_grad()
        score.backward()
        
        # è·å–ç‰¹å¾å’Œæ¢¯åº¦
        if self.features is None or self.gradients is None:
            print("âŒ æ— æ³•è·å–ç‰¹å¾æˆ–æ¢¯åº¦")
            return None
        
        # è®¡ç®—æƒé‡
        weights = torch.mean(self.gradients, dim=(2, 3))
        
        # ç”ŸæˆCAM
        cam = torch.zeros(self.features.shape[2:], dtype=torch.float32, device=self.device)
        
        for i, w in enumerate(weights[0]):
            cam += w * self.features[0, i, :, :]
        
        # åº”ç”¨ReLU
        cam = F.relu(cam)
        
        # å½’ä¸€åŒ–
        cam = cam - cam.min()
        cam = cam / (cam.max() + 1e-8)
        
        return cam.detach().cpu().numpy()
    
    def visualize_cam(self, image, cam, save_path=None, alpha=0.6, title="CAM Visualization"):
        """å¯è§†åŒ–CAM"""
        # ç¡®ä¿å›¾åƒæ˜¯numpyæ•°ç»„
        if isinstance(image, torch.Tensor):
            image = image.squeeze(0).cpu().numpy()
            if image.shape[0] == 3:  # CHW -> HWC
                image = np.transpose(image, (1, 2, 0))
        
        # è°ƒæ•´CAMå¤§å°ä»¥åŒ¹é…å›¾åƒ
        cam_resized = cv2.resize(cam, (image.shape[1], image.shape[0]))
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)
        heatmap = np.float32(heatmap) / 255
        
        # è½¬æ¢å›¾åƒæ ¼å¼
        if image.max() <= 1.0:
            image = np.uint8(255 * image)
        else:
            image = np.uint8(image)
        
        # å åŠ 
        cam_image = heatmap + alpha * np.float32(image) / 255
        cam_image = cam_image / np.max(cam_image)
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # åŸå§‹å›¾åƒ
        axes[0].imshow(image)
        axes[0].set_title('Original Image', fontsize=12)
        axes[0].axis('off')
        
        # CAMçƒ­åŠ›å›¾
        axes[1].imshow(cam_resized, cmap='jet')
        axes[1].set_title('CAM Heatmap', fontsize=12)
        axes[1].axis('off')
        
        # å åŠ ç»“æœ
        axes[2].imshow(cam_image)
        axes[2].set_title('CAM Overlay', fontsize=12)
        axes[2].axis('off')
        
        plt.suptitle(title, fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"âœ… CAMå›¾ç‰‡å·²ä¿å­˜: {save_path}")
        
        plt.show()
        
        return cam_image

def load_gtsrb_samples_detailed(gtsrb_path, samples_per_combo=3):
    """åŠ è½½è¯¦ç»†çš„GTSRBæ ·æœ¬"""
    samples = []
    
    if not os.path.exists(gtsrb_path):
        print(f"âŒ GTSRBè·¯å¾„ä¸å­˜åœ¨: {gtsrb_path}")
        return samples
    
    # ä¸ºæ¯ä¸ªé¢œè‰²ç»„åˆåŠ è½½å¤šä¸ªæ ·æœ¬
    for combo_id in range(5):
        combo = IMPROVED_COLOR_COMBINATIONS[combo_id]
        classes = combo['classes']
        
        print(f"ğŸ“¸ åŠ è½½ {combo['name']} ç»„åˆçš„æ ·æœ¬...")
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åŠ è½½æ ·æœ¬
        for class_id in classes[:3]:  # æ¯ä¸ªç»„åˆæœ€å¤š3ä¸ªç±»åˆ«
            class_folder = os.path.join(gtsrb_path, 'Final_Training', 'Images', f'{class_id:05d}')
            
            if os.path.exists(class_folder):
                # è·å–è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰å›¾åƒ
                image_files = [f for f in os.listdir(class_folder) if f.endswith('.ppm')]
                
                # éšæœºé€‰æ‹©æ ·æœ¬
                selected_files = random.sample(image_files, min(samples_per_combo, len(image_files)))
                
                for img_name in selected_files:
                    img_path = os.path.join(class_folder, img_name)
                    try:
                        img = Image.open(img_path).convert('RGB')
                        img = img.resize((32, 32))
                        img_array = np.array(img).astype(np.float32) / 255.0
                        
                        samples.append({
                            'image': img_array,
                            'label': combo_id,
                            'name': f"{combo['name']} (GTSRB {class_id})",
                            'class_id': class_id,
                            'combo_id': combo_id,
                            'file_name': img_name
                        })
                    except Exception as e:
                        print(f"âŒ åŠ è½½å›¾åƒå¤±è´¥ {img_path}: {e}")
                        continue
    
    return samples

def analyze_cam_results(cam_generator, samples, output_dir):
    """åˆ†æCAMç»“æœ"""
    results = []
    
    for i, sample in enumerate(samples):
        print(f"\nğŸ¯ åˆ†ææ ·æœ¬ {i+1}/{len(samples)}: {sample['name']}")
        
        # ç”ŸæˆCAM
        cam = cam_generator.generate_cam(sample['image'], sample['label'])
        
        if cam is not None:
            # ä¿å­˜è·¯å¾„
            save_path = os.path.join(output_dir, f'cam_{sample["name"]}_{sample["file_name"]}.png')
            
            # å¯è§†åŒ–
            cam_generator.visualize_cam(sample['image'], cam, save_path, 
                                      title=f"{sample['name']} - {sample['file_name']}")
            
            # ä¿å­˜åŸå§‹CAMæ•°æ®
            cam_data_path = os.path.join(output_dir, f'cam_{sample["name"]}_{sample["file_name"]}.npy')
            np.save(cam_data_path, cam)
            
            # åˆ†æCAM
            cam_analysis = analyze_single_cam(cam, sample)
            results.append(cam_analysis)
            
            print(f"ğŸ’¾ CAMæ•°æ®å·²ä¿å­˜: {cam_data_path}")
        else:
            print(f"âŒ æ— æ³•ä¸º {sample['name']} ç”ŸæˆCAM")
    
    return results

def analyze_single_cam(cam, sample):
    """åˆ†æå•ä¸ªCAM"""
    # è®¡ç®—CAMçš„ç»Ÿè®¡ä¿¡æ¯
    cam_mean = np.mean(cam)
    cam_std = np.std(cam)
    cam_max = np.max(cam)
    cam_min = np.min(cam)
    
    # è®¡ç®—æ³¨æ„åŠ›é›†ä¸­åº¦ï¼ˆé«˜å€¼åŒºåŸŸçš„æ¯”ä¾‹ï¼‰
    attention_threshold = 0.5
    high_attention_ratio = np.sum(cam > attention_threshold) / cam.size
    
    # è®¡ç®—ç©ºé—´åˆ†å¸ƒ
    center_x, center_y = cam.shape[1] // 2, cam.shape[0] // 2
    center_attention = np.mean(cam[center_y-4:center_y+4, center_x-4:center_x+4])
    
    return {
        'sample_name': sample['name'],
        'class_id': sample['class_id'],
        'combo_id': sample['combo_id'],
        'file_name': sample['file_name'],
        'cam_stats': {
            'mean': float(cam_mean),
            'std': float(cam_std),
            'max': float(cam_max),
            'min': float(cam_min),
            'high_attention_ratio': float(high_attention_ratio),
            'center_attention': float(center_attention)
        }
    }

def main():
    parser = argparse.ArgumentParser(description='ç”ŸæˆçœŸå®GTSRBå›¾ç‰‡çš„CAMå¯è§†åŒ–')
    parser.add_argument('--model_path', default='best_improved_color_combination_head.pth', 
                       help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--gtsrb_path', default='/home/hding22/color/GTSRB/GTSRB',
                       help='GTSRBæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output_dir', default='real_gtsrb_cam_detailed',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--samples_per_combo', type=int, default=3,
                       help='æ¯ä¸ªé¢œè‰²ç»„åˆçš„æ ·æœ¬æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # åˆå§‹åŒ–CAMç”Ÿæˆå™¨
    cam_generator = RealGTSRBCAMGenerator(args.model_path)
    
    # åŠ è½½çœŸå®GTSRBæ ·æœ¬
    print("ğŸ“¸ åŠ è½½çœŸå®GTSRBæ ·æœ¬...")
    samples = load_gtsrb_samples_detailed(args.gtsrb_path, args.samples_per_combo)
    
    print(f"âœ… å‡†å¤‡ç”Ÿæˆ {len(samples)} ä¸ªçœŸå®GTSRBæ ·æœ¬çš„CAMå¯è§†åŒ–")
    
    # åˆ†æCAMç»“æœ
    results = analyze_cam_results(cam_generator, samples, args.output_dir)
    
    print(f"\nğŸ‰ çœŸå®GTSRB CAMå¯è§†åŒ–å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    
    # ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
    analysis_report = {
        'model_path': args.model_path,
        'output_dir': args.output_dir,
        'num_samples': len(samples),
        'data_type': 'Real GTSRB',
        'samples_per_combo': args.samples_per_combo,
        'cam_analysis': results,
        'summary_stats': generate_summary_stats(results)
    }
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    report_path = os.path.join(args.output_dir, 'detailed_cam_analysis.json')
    with open(report_path, 'w') as f:
        json.dump(analysis_report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # ç”Ÿæˆå¯è§†åŒ–æ€»ç»“
    generate_visualization_summary(results, args.output_dir)

def generate_summary_stats(results):
    """ç”Ÿæˆæ€»ç»“ç»Ÿè®¡"""
    if not results:
        return {}
    
    # æŒ‰é¢œè‰²ç»„åˆåˆ†ç»„
    combo_stats = defaultdict(list)
    for result in results:
        combo_id = result['combo_id']
        combo_stats[combo_id].append(result['cam_stats'])
    
    summary = {}
    for combo_id, stats_list in combo_stats.items():
        combo_name = IMPROVED_COLOR_COMBINATIONS[combo_id]['name']
        summary[combo_name] = {
            'num_samples': len(stats_list),
            'avg_attention_ratio': np.mean([s['high_attention_ratio'] for s in stats_list]),
            'avg_center_attention': np.mean([s['center_attention'] for s in stats_list]),
            'avg_cam_mean': np.mean([s['mean'] for s in stats_list]),
            'avg_cam_std': np.mean([s['std'] for s in stats_list])
        }
    
    return summary

def generate_visualization_summary(results, output_dir):
    """ç”Ÿæˆå¯è§†åŒ–æ€»ç»“"""
    if not results:
        return
    
    # åˆ›å»ºæ€»ç»“å›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æŒ‰é¢œè‰²ç»„åˆåˆ†ç»„çš„æ³¨æ„åŠ›æ¯”ä¾‹
    combo_attention = defaultdict(list)
    for result in results:
        combo_name = IMPROVED_COLOR_COMBINATIONS[result['combo_id']]['name']
        combo_attention[combo_name].append(result['cam_stats']['high_attention_ratio'])
    
    # ç»˜åˆ¶æ³¨æ„åŠ›æ¯”ä¾‹ç®±çº¿å›¾
    attention_data = [combo_attention[name] for name in combo_attention.keys()]
    axes[0, 0].boxplot(attention_data, labels=list(combo_attention.keys()))
    axes[0, 0].set_title('Attention Ratio by Color Combination')
    axes[0, 0].set_ylabel('High Attention Ratio')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # ç»˜åˆ¶ä¸­å¿ƒæ³¨æ„åŠ›åˆ†å¸ƒ
    center_attention = [r['cam_stats']['center_attention'] for r in results]
    axes[0, 1].hist(center_attention, bins=20, alpha=0.7)
    axes[0, 1].set_title('Center Attention Distribution')
    axes[0, 1].set_xlabel('Center Attention')
    axes[0, 1].set_ylabel('Frequency')
    
    # ç»˜åˆ¶CAMå‡å€¼åˆ†å¸ƒ
    cam_means = [r['cam_stats']['mean'] for r in results]
    axes[1, 0].hist(cam_means, bins=20, alpha=0.7, color='green')
    axes[1, 0].set_title('CAM Mean Distribution')
    axes[1, 0].set_xlabel('CAM Mean')
    axes[1, 0].set_ylabel('Frequency')
    
    # ç»˜åˆ¶CAMæ ‡å‡†å·®åˆ†å¸ƒ
    cam_stds = [r['cam_stats']['std'] for r in results]
    axes[1, 1].hist(cam_stds, bins=20, alpha=0.7, color='red')
    axes[1, 1].set_title('CAM Standard Deviation Distribution')
    axes[1, 1].set_xlabel('CAM Standard Deviation')
    axes[1, 1].set_ylabel('Frequency')
    
    plt.tight_layout()
    
    # ä¿å­˜æ€»ç»“å›¾è¡¨
    summary_path = os.path.join(output_dir, 'cam_analysis_summary.png')
    plt.savefig(summary_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š åˆ†ææ€»ç»“å›¾è¡¨å·²ä¿å­˜: {summary_path}")
    
    plt.show()

if __name__ == "__main__":
    main()
