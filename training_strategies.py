# training_strategies.py
# è®­ç»ƒç­–ç•¥æ¨¡å— - åˆ†é˜¶æ®µè®­ç»ƒå’Œè¯¾ç¨‹å­¦ä¹ 

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import numpy as np
from typing import Dict, List, Tuple, Callable
import time

class StagedTraining:
    """åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥"""
    
    def __init__(self, model: nn.Module, train_loader: DataLoader, 
                 val_loader: DataLoader, device: torch.device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # è®­ç»ƒé˜¶æ®µé…ç½®
        self.stages = {
            'color_detector': {
                'epochs': 5,
                'lr': 0.001,
                'description': 'é¢„è®­ç»ƒé¢œè‰²æ£€æµ‹å™¨'
            },
            'shape_decorrelation': {
                'epochs': 5,
                'lr': 0.0005,
                'description': 'è®­ç»ƒå½¢çŠ¶å»ç›¸å…³'
            },
            'joint_training': {
                'epochs': 10,
                'lr': 0.0001,
                'description': 'è”åˆè®­ç»ƒ'
            }
        }
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'stages': {},
            'best_val_acc': 0,
            'best_epoch': 0
        }
    
    def train_color_detector_only(self, epochs: int, lr: float):
        """é˜¶æ®µ1: åªè®­ç»ƒé¢œè‰²æ£€æµ‹å™¨"""
        print(f"ğŸ¨ é˜¶æ®µ1: é¢„è®­ç»ƒé¢œè‰²æ£€æµ‹å™¨ ({epochs} epochs, lr={lr})")
        
        # å†»ç»“å…¶ä»–ç»„ä»¶
        for name, param in self.model.named_parameters():
            if 'color_detector' not in name and 'color_feature_extractor' not in name:
                param.requires_grad = False
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=lr, weight_decay=1e-4
        )
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            
            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(data)
                
                # åªè®¡ç®—é¢œè‰²æ£€æµ‹æŸå¤±
                if hasattr(self.model, 'compute_enhanced_loss'):
                    loss_dict = self.model.compute_enhanced_loss(outputs, target, data)
                    loss = loss_dict['color_consistency_loss'] + loss_dict['color_invariance_loss']
                else:
                    loss = nn.CrossEntropyLoss()(outputs['color_logits'], target)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 50 == 0:
                    print(f"  Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
            
            # éªŒè¯
            val_acc = self._validate_model()
            print(f"  Epoch {epoch+1} - Avg Loss: {total_loss/len(self.train_loader):.4f}, Val Acc: {val_acc:.4f}")
        
        # è§£å†»æ‰€æœ‰å‚æ•°
        for param in self.model.parameters():
            param.requires_grad = True
    
    def train_shape_decorrelation(self, epochs: int, lr: float):
        """é˜¶æ®µ2: è®­ç»ƒå½¢çŠ¶å»ç›¸å…³"""
        print(f"ğŸ”„ é˜¶æ®µ2: è®­ç»ƒå½¢çŠ¶å»ç›¸å…³ ({epochs} epochs, lr={lr})")
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            
            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(data)
                
                # ä¸»è¦å…³æ³¨å½¢çŠ¶å»ç›¸å…³æŸå¤±
                if hasattr(self.model, 'compute_enhanced_loss'):
                    loss_dict = self.model.compute_enhanced_loss(outputs, target, data)
                    loss = (
                        loss_dict['shape_decorr_loss'] * 2.0 +  # å¢åŠ æƒé‡
                        loss_dict['color_consistency_loss'] * 0.5 +
                        loss_dict['ce_loss'] * 0.5
                    )
                else:
                    loss = nn.CrossEntropyLoss()(outputs['logits'], target)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 50 == 0:
                    print(f"  Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
            
            # éªŒè¯
            val_acc = self._validate_model()
            print(f"  Epoch {epoch+1} - Avg Loss: {total_loss/len(self.train_loader):.4f}, Val Acc: {val_acc:.4f}")
    
    def train_joint_model(self, epochs: int, lr: float):
        """é˜¶æ®µ3: è”åˆè®­ç»ƒ"""
        print(f"ğŸ¤ é˜¶æ®µ3: è”åˆè®­ç»ƒ ({epochs} epochs, lr={lr})")
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
        
        best_val_acc = 0
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            
            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(data)
                
                # å®Œæ•´æŸå¤±
                if hasattr(self.model, 'compute_enhanced_loss'):
                    loss_dict = self.model.compute_enhanced_loss(outputs, target, data)
                    loss = loss_dict['total_loss']
                else:
                    loss = nn.CrossEntropyLoss()(outputs['logits'], target)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 50 == 0:
                    print(f"  Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # éªŒè¯
            val_acc = self._validate_model()
            print(f"  Epoch {epoch+1} - Avg Loss: {total_loss/len(self.train_loader):.4f}, Val Acc: {val_acc:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(self.model.state_dict(), f'checkpoints/best_staged_model_epoch_{epoch+1}.pth')
        
        return best_val_acc
    
    def _validate_model(self) -> float:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = self.model(data)
                
                if 'final_logits' in outputs:
                    predictions = torch.argmax(outputs['final_logits'], dim=1)
                elif 'logits' in outputs:
                    predictions = torch.argmax(outputs['logits'], dim=1)
                else:
                    predictions = torch.argmax(outputs, dim=1)
                
                correct += (predictions == target).sum().item()
                total += target.size(0)
        
        return correct / total
    
    def run_staged_training(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†é˜¶æ®µè®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹åˆ†é˜¶æ®µè®­ç»ƒ")
        print("="*60)
        
        start_time = time.time()
        
        for stage_name, config in self.stages.items():
            print(f"\nğŸ“‹ {config['description']}")
            print("-" * 40)
            
            if stage_name == 'color_detector':
                self.train_color_detector_only(config['epochs'], config['lr'])
            elif stage_name == 'shape_decorrelation':
                self.train_shape_decorrelation(config['epochs'], config['lr'])
            elif stage_name == 'joint_training':
                best_acc = self.train_joint_model(config['epochs'], config['lr'])
                self.training_history['best_val_acc'] = best_acc
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ åˆ†é˜¶æ®µè®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time/60:.2f}åˆ†é’Ÿ")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.training_history['best_val_acc']:.4f}")

class CurriculumLearning:
    """è¯¾ç¨‹å­¦ä¹ ç­–ç•¥"""
    
    def __init__(self, model: nn.Module, train_loader: DataLoader, 
                 val_loader: DataLoader, device: torch.device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # è¯¾ç¨‹é…ç½®
        self.curriculum = {
            'single_color': {
                'classes': [14, 17, 33, 34, 35],  # Stop, No Entry, Turn Right, Turn Left, Ahead Only
                'epochs': 3,
                'description': 'å•è‰²æ ‡å¿—å­¦ä¹ '
            },
            'dual_color': {
                'classes': [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11],  # Speed limits
                'epochs': 3,
                'description': 'åŒè‰²æ ‡å¿—å­¦ä¹ '
            },
            'multi_color': {
                'classes': [12, 13, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],  # å¤æ‚æ ‡å¿—
                'epochs': 3,
                'description': 'å¤šè‰²æ ‡å¿—å­¦ä¹ '
            },
            'all_classes': {
                'classes': list(range(43)),  # æ‰€æœ‰ç±»åˆ«
                'epochs': 10,
                'description': 'å…¨ç±»åˆ«è®­ç»ƒ'
            }
        }
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'curriculum': {},
            'best_val_acc': 0,
            'best_epoch': 0
        }
    
    def create_subset_loader(self, class_indices: List[int]) -> DataLoader:
        """åˆ›å»ºç‰¹å®šç±»åˆ«çš„æ•°æ®åŠ è½½å™¨"""
        # è·å–æŒ‡å®šç±»åˆ«çš„æ ·æœ¬ç´¢å¼•
        subset_indices = []
        for idx, (_, target) in enumerate(self.train_loader.dataset):
            if target in class_indices:
                subset_indices.append(idx)
        
        # åˆ›å»ºå­é›†
        subset = Subset(self.train_loader.dataset, subset_indices)
        subset_loader = DataLoader(
            subset, 
            batch_size=self.train_loader.batch_size,
            shuffle=True,
            num_workers=self.train_loader.num_workers
        )
        
        return subset_loader
    
    def train_on_subset(self, class_indices: List[int], epochs: int, lr: float, stage_name: str):
        """åœ¨ç‰¹å®šç±»åˆ«å­é›†ä¸Šè®­ç»ƒ"""
        print(f"ğŸ“š {stage_name}: è®­ç»ƒç±»åˆ« {class_indices} ({epochs} epochs, lr={lr})")
        
        # åˆ›å»ºå­é›†åŠ è½½å™¨
        subset_loader = self.create_subset_loader(class_indices)
        print(f"  å­é›†å¤§å°: {len(subset_loader.dataset)} æ ·æœ¬")
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_idx, (data, target) in enumerate(subset_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(data)
                
                # è®¡ç®—æŸå¤±
                if hasattr(self.model, 'compute_enhanced_loss'):
                    loss_dict = self.model.compute_enhanced_loss(outputs, target, data)
                    loss = loss_dict['total_loss']
                else:
                    loss = nn.CrossEntropyLoss()(outputs['logits'], target)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                if 'final_logits' in outputs:
                    predictions = torch.argmax(outputs['final_logits'], dim=1)
                elif 'logits' in outputs:
                    predictions = torch.argmax(outputs['logits'], dim=1)
                else:
                    predictions = torch.argmax(outputs, dim=1)
                
                correct += (predictions == target).sum().item()
                total += target.size(0)
                
                if batch_idx % 20 == 0:
                    print(f"    Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
            
            # è®¡ç®—å­é›†å‡†ç¡®ç‡
            subset_acc = correct / total
            avg_loss = total_loss / len(subset_loader)
            print(f"    Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}, Subset Acc: {subset_acc:.4f}")
            
            # åœ¨å®Œæ•´éªŒè¯é›†ä¸ŠéªŒè¯
            val_acc = self._validate_model()
            print(f"    Epoch {epoch+1} - Val Acc: {val_acc:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.training_history['best_val_acc']:
                self.training_history['best_val_acc'] = val_acc
                torch.save(self.model.state_dict(), f'checkpoints/best_curriculum_{stage_name}_epoch_{epoch+1}.pth')
    
    def _validate_model(self) -> float:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = self.model(data)
                
                if 'final_logits' in outputs:
                    predictions = torch.argmax(outputs['final_logits'], dim=1)
                elif 'logits' in outputs:
                    predictions = torch.argmax(outputs['logits'], dim=1)
                else:
                    predictions = torch.argmax(outputs, dim=1)
                
                correct += (predictions == target).sum().item()
                total += target.size(0)
        
        return correct / total
    
    def run_curriculum_learning(self):
        """è¿è¡Œè¯¾ç¨‹å­¦ä¹ """
        print("ğŸ“š å¼€å§‹è¯¾ç¨‹å­¦ä¹ ")
        print("="*60)
        
        start_time = time.time()
        
        for stage_name, config in self.curriculum.items():
            print(f"\nğŸ“‹ {config['description']}")
            print("-" * 40)
            
            self.train_on_subset(
                config['classes'], 
                config['epochs'], 
                0.001,  # å›ºå®šå­¦ä¹ ç‡
                stage_name
            )
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è¯¾ç¨‹å­¦ä¹ å®Œæˆ! æ€»æ—¶é—´: {total_time/60:.2f}åˆ†é’Ÿ")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.training_history['best_val_acc']:.4f}")

def test_training_strategies():
    """æµ‹è¯•è®­ç»ƒç­–ç•¥æ¨¡å—"""
    print("ğŸ¯ æµ‹è¯•è®­ç»ƒç­–ç•¥æ¨¡å—")
    print("="*60)
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.features = nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, 43)
            )
        
        def forward(self, x):
            return {'logits': self.features(x)}
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    class TestDataset:
        def __init__(self, size=1000):
            self.size = size
            self.data = torch.randn(size, 64)
            self.targets = torch.randint(0, 43, (size,))
        
        def __getitem__(self, idx):
            return self.data[idx], self.targets[idx]
        
        def __len__(self):
            return self.size
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TestDataset()
    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = TestModel().to(device)
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•åˆ†é˜¶æ®µè®­ç»ƒ
    print(f"\nğŸ§ª æµ‹è¯•åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥:")
    staged_trainer = StagedTraining(model, train_loader, val_loader, device)
    
    # æµ‹è¯•è¯¾ç¨‹å­¦ä¹ 
    print(f"\nğŸ§ª æµ‹è¯•è¯¾ç¨‹å­¦ä¹ ç­–ç•¥:")
    curriculum_trainer = CurriculumLearning(model, train_loader, val_loader, device)
    
    print(f"\nâœ… è®­ç»ƒç­–ç•¥æ¨¡å—æµ‹è¯•å®Œæˆ")

if __name__ == '__main__':
    test_training_strategies()
