# train_with_gtsrb.py
# ä½¿ç”¨çœŸå®GTSRBæ•°æ®é›†è®­ç»ƒå¢å¼ºé¢œè‰²å¤´

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import sys
import os
import time
import numpy as np
from typing import Dict, List, Tuple

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from datasets.gtsrb_dataset import get_gtsrb_dataloaders
    from stable_training_model import StableTrainingModel
    from gtsrb_symbolic_knowledge import GTSRBSymbolicKnowledge
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥æŸäº›æ¨¡å—: {e}")
    print("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")

from complete_enhanced_color_head import CompleteEnhancedColorHead as EnhancedColorSemanticHead
from color_augmentation import AdvancedColorSpecificAugmentation as ColorSpecificAugmentation, ColorRobustnessLoss
from training_strategies import StagedTraining, CurriculumLearning

class GTSRBEnhancedColorModel(nn.Module):
    """GTSRBå¢å¼ºé¢œè‰²æ¨¡å‹ - æ•´åˆæ‰€æœ‰æ”¹è¿›"""
    
    def __init__(self, base_model=None):
        super().__init__()
        
        # å¦‚æœæ²¡æœ‰æä¾›åŸºç¡€æ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„
        if base_model is None:
            self.base_model = SimpleBaseModel()
        else:
            self.base_model = base_model
        
        # è·å–ç‰¹å¾ç»´åº¦
        if hasattr(self.base_model, 'prototype_features'):
            feature_dim = self.base_model.prototype_features.size(1)
        else:
            feature_dim = 1161  # é»˜è®¤ç»´åº¦
        
        # å¢å¼ºé¢œè‰²å¤´
        self.enhanced_color_head = EnhancedColorSemanticHead(
            input_dim=feature_dim, 
            num_classes=43
        )
        
        # é¢œè‰²å¢å¼º
        self.color_augmentation = ColorSpecificAugmentation()
        self.color_robustness_loss = ColorRobustnessLoss()
        
        # èåˆæƒé‡
        self.fusion_weight = nn.Parameter(torch.tensor(0.5))
        
        # ç¬¦å·çŸ¥è¯†
        self.gtsrb_knowledge = GTSRBSymbolicKnowledge()
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # åŸºç¡€æ¨¡å‹å‰å‘ä¼ æ’­
        base_outputs = self.base_model(x)
        
        # è·å–ç‰¹å¾
        if 'features' in base_outputs:
            features = base_outputs['features']
        elif 'prototype_features' in base_outputs:
            features = base_outputs['prototype_features']
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹å¾ï¼Œä½¿ç”¨è¾“å…¥
            features = x.view(x.size(0), -1)
        
        # å¢å¼ºé¢œè‰²å¤´å‰å‘ä¼ æ’­
        color_outputs = self.enhanced_color_head(features)
        
        # èåˆé¢„æµ‹
        if 'logits' in base_outputs:
            base_logits = base_outputs['logits']
        else:
            base_logits = torch.zeros(x.size(0), 43, device=x.device)
        
        color_logits = color_outputs['color_semantic_logits']
        
        # è‡ªé€‚åº”èåˆ
        fusion_weight = torch.sigmoid(self.fusion_weight)
        final_logits = fusion_weight * color_logits + (1 - fusion_weight) * base_logits
        
        return {
            'base_logits': base_logits,
            'color_logits': color_logits,
            'final_logits': final_logits,
            'color_outputs': color_outputs,
            'base_outputs': base_outputs,
            'fusion_weight': fusion_weight,
            'features': features
        }
    
    def compute_enhanced_loss(self, outputs: Dict[str, torch.Tensor], 
                            targets: torch.Tensor, 
                            inputs: torch.Tensor) -> Dict[str, torch.Tensor]:
        """è®¡ç®—å¢å¼ºæŸå¤±"""
        # åŸºç¡€åˆ†ç±»æŸå¤±
        ce_loss = nn.CrossEntropyLoss()(outputs['final_logits'], targets)
        
        # é¢œè‰²å¤´æŸå¤±
        color_loss_dict = self.enhanced_color_head.compute_complete_loss(
            outputs['color_outputs'], targets, inputs
        )
        
        # èåˆæŸå¤±
        fusion_loss = nn.MSELoss()(outputs['final_logits'], outputs['base_logits'])
        
        # é¢œè‰²é²æ£’æ€§æŸå¤±
        if hasattr(self, 'color_robustness_loss'):
            robustness_loss = self.color_robustness_loss(
                outputs['features'], outputs['features'], 
                outputs['final_logits'], outputs['final_logits']
            )
            robustness_total = robustness_loss['total_loss']
        else:
            robustness_total = torch.tensor(0.0, device=outputs['final_logits'].device)
        
        # æ€»æŸå¤±
        total_loss = (
            ce_loss + 
            color_loss_dict['total_loss'] * 0.5 +
            fusion_loss * 0.1 +
            robustness_total * 0.05
        )
        
        return {
            'total_loss': total_loss,
            'ce_loss': ce_loss,
            'color_loss': color_loss_dict['total_loss'],
            'fusion_loss': fusion_loss,
            'robustness_loss': robustness_total
        }

class SimpleBaseModel(nn.Module):
    """ç®€å•çš„åŸºç¡€æ¨¡å‹ - ç”¨äºæµ‹è¯•"""
    
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Linear(3072, 512),  # 32x32x3 = 3072
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        self.classifier = nn.Linear(128, 43)
        self.prototype_features = torch.randn(43, 128)  # æ¨¡æ‹ŸåŸå‹ç‰¹å¾
    
    def forward(self, x):
        # å±•å¹³è¾“å…¥
        x_flat = x.view(x.size(0), -1)
        features = self.features(x_flat)
        logits = self.classifier(features)
        
        return {
            'logits': logits,
            'features': features,
            'prototype_features': features
        }

def create_mock_dataloaders():
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨"""
    print("ğŸ“Š åˆ›å»ºæ¨¡æ‹ŸGTSRBæ•°æ®åŠ è½½å™¨")
    
    class MockGTSRBDataset:
        def __init__(self, size=1000, train=True):
            self.size = size
            self.train = train
            # åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒæ•°æ®
            self.data = torch.randn(size, 3, 32, 32)
            self.targets = torch.randint(0, 43, (size,))
        
        def __getitem__(self, idx):
            return self.data[idx], self.targets[idx]
        
        def __len__(self):
            return self.size
    
    train_dataset = MockGTSRBDataset(size=2000, train=True)
    test_dataset = MockGTSRBDataset(size=500, train=False)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)
    
    return train_loader, test_loader

def train_enhanced_model_with_gtsrb(
    epochs: int = 20,
    batch_size: int = 32,
    lr: float = 0.001,
    use_real_data: bool = False,
    model_name: str = "gtsrb_enhanced_color_model"
):
    """ä½¿ç”¨GTSRBæ•°æ®é›†è®­ç»ƒå¢å¼ºé¢œè‰²æ¨¡å‹"""
    
    print("ğŸ¨ GTSRBå¢å¼ºé¢œè‰²æ¨¡å‹è®­ç»ƒ")
    print("="*60)
    print(f"è®­ç»ƒé…ç½®:")
    print(f"  epochs: {epochs}")
    print(f"  batch_size: {batch_size}")
    print(f"  lr: {lr}")
    print(f"  use_real_data: {use_real_data}")
    print(f"  model_name: {model_name}")
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®åŠ è½½å™¨
    if use_real_data:
        try:
            print("\nğŸ“Š åŠ è½½çœŸå®GTSRBæ•°æ®é›†...")
            train_loader, test_loader = get_gtsrb_dataloaders(
                data_root='./data/GTSRB',
                batch_size=batch_size,
                num_workers=4,
                use_augmentation=True
            )
            print("âœ… çœŸå®GTSRBæ•°æ®é›†åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ çœŸå®æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            print("ğŸ“Š ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é›†...")
            train_loader, test_loader = create_mock_dataloaders()
    else:
        print("ğŸ“Š ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®é›†...")
        train_loader, test_loader = create_mock_dataloaders()
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸ åˆ›å»ºå¢å¼ºé¢œè‰²æ¨¡å‹...")
    enhanced_model = GTSRBEnhancedColorModel().to(device)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in enhanced_model.parameters())
    trainable_params = sum(p.numel() for p in enhanced_model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.AdamW(enhanced_model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
    
    # è®­ç»ƒå†å²
    train_history = {
        'loss': [], 'accuracy': [], 'color_accuracy': [], 'fusion_weight': []
    }
    val_history = {
        'loss': [], 'accuracy': [], 'color_accuracy': []
    }
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    best_val_acc = 0
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        enhanced_model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        color_correct = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = enhanced_model(data)
            loss_dict = enhanced_model.compute_enhanced_loss(outputs, target, data)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss_dict['total_loss'].backward()
            torch.nn.utils.clip_grad_norm_(enhanced_model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # ç»Ÿè®¡
            train_loss += loss_dict['total_loss'].item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs['final_logits'], 1)
            train_total += target.size(0)
            train_correct += (predicted == target).sum().item()
            
            # é¢œè‰²å¤´å‡†ç¡®ç‡
            _, color_predicted = torch.max(outputs['color_logits'], 1)
            color_correct += (color_predicted == target).sum().item()
            
            if batch_idx % 50 == 0:
                print(f"  Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss_dict['total_loss'].item():.4f}")
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # éªŒè¯é˜¶æ®µ
        enhanced_model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        val_color_correct = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                outputs = enhanced_model(data)
                loss_dict = enhanced_model.compute_enhanced_loss(outputs, target, data)
                
                val_loss += loss_dict['total_loss'].item()
                
                _, predicted = torch.max(outputs['final_logits'], 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()
                
                _, color_predicted = torch.max(outputs['color_logits'], 1)
                val_color_correct += (color_predicted == target).sum().item()
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        train_color_acc = 100. * color_correct / train_total
        val_color_acc = 100. * val_color_correct / val_total
        
        # è®°å½•å†å²
        train_history['loss'].append(train_loss / len(train_loader))
        train_history['accuracy'].append(train_acc)
        train_history['color_accuracy'].append(train_color_acc)
        train_history['fusion_weight'].append(outputs['fusion_weight'].item())
        
        val_history['loss'].append(val_loss / len(test_loader))
        val_history['accuracy'].append(val_acc)
        val_history['color_accuracy'].append(val_color_acc)
        
        # æ‰“å°ç»“æœ
        print(f"Epoch {epoch+1}/{epochs}:")
        print(f"  Train - Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%, Color Acc: {train_color_acc:.2f}%")
        print(f"  Val   - Loss: {val_loss/len(test_loader):.4f}, Acc: {val_acc:.2f}%, Color Acc: {val_color_acc:.2f}%")
        print(f"  Fusion Weight: {outputs['fusion_weight'].item():.3f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': enhanced_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_acc': best_val_acc,
                'train_history': train_history,
                'val_history': val_history
            }, f'checkpoints/{model_name}_best.pth')
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {best_val_acc:.2f}%)")
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - start_time
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.2f}åˆ†é’Ÿ")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    return enhanced_model, train_history, val_history

def evaluate_enhanced_model(model: GTSRBEnhancedColorModel, test_loader: DataLoader):
    """è¯„ä¼°å¢å¼ºæ¨¡å‹"""
    print("\nğŸ“Š è¯„ä¼°å¢å¼ºé¢œè‰²æ¨¡å‹")
    print("="*40)
    
    device = next(model.parameters()).device
    model.eval()
    
    total_correct = 0
    total_samples = 0
    color_correct = 0
    base_correct = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            
            # æœ€ç»ˆé¢„æµ‹
            _, predicted = torch.max(outputs['final_logits'], 1)
            total_correct += (predicted == target).sum().item()
            
            # é¢œè‰²å¤´é¢„æµ‹
            _, color_predicted = torch.max(outputs['color_logits'], 1)
            color_correct += (color_predicted == target).sum().item()
            
            # åŸºç¡€æ¨¡å‹é¢„æµ‹
            _, base_predicted = torch.max(outputs['base_logits'], 1)
            base_correct += (base_predicted == target).sum().item()
            
            total_samples += target.size(0)
    
    final_acc = 100. * total_correct / total_samples
    color_acc = 100. * color_correct / total_samples
    base_acc = 100. * base_correct / total_samples
    
    print(f"æœ€ç»ˆå‡†ç¡®ç‡: {final_acc:.2f}%")
    print(f"é¢œè‰²å¤´å‡†ç¡®ç‡: {color_acc:.2f}%")
    print(f"åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡: {base_acc:.2f}%")
    print(f"èåˆæƒé‡: {outputs['fusion_weight'].item():.3f}")
    
    return {
        'final_accuracy': final_acc,
        'color_accuracy': color_acc,
        'base_accuracy': base_acc,
        'fusion_weight': outputs['fusion_weight'].item()
    }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ GTSRBå¢å¼ºé¢œè‰²æ¨¡å‹è®­ç»ƒè„šæœ¬")
    print("="*60)
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs('checkpoints', exist_ok=True)
    
    # è®­ç»ƒé…ç½®
    config = {
        'epochs': 15,
        'batch_size': 32,
        'lr': 0.001,
        'use_real_data': False,  # è®¾ç½®ä¸ºTrueä½¿ç”¨çœŸå®GTSRBæ•°æ®é›†
        'model_name': 'gtsrb_enhanced_color_model'
    }
    
    # è®­ç»ƒæ¨¡å‹
    enhanced_model, train_history, val_history = train_enhanced_model_with_gtsrb(**config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨è¿›è¡Œè¯„ä¼°
    _, test_loader = create_mock_dataloaders()
    
    # è¯„ä¼°æ¨¡å‹
    evaluation_results = evaluate_enhanced_model(enhanced_model, test_loader)
    
    print(f"\nğŸ“ˆ è®­ç»ƒæ€»ç»“:")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(val_history['accuracy']):.2f}%")
    print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {evaluation_results['final_accuracy']:.2f}%")
    print(f"é¢œè‰²å¤´è´¡çŒ®: {evaluation_results['color_accuracy']:.2f}%")
    print(f"èåˆæƒé‡: {evaluation_results['fusion_weight']:.3f}")

if __name__ == '__main__':
    main()
