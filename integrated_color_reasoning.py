import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import json

# å¯¼å…¥ç°æœ‰æ¨¡å—
from improved_color_combination import ImprovedColorCombinationHead
from color_relation_reasoning import ColorRelationReasoning

class IntegratedColorReasoningModel(nn.Module):
    """é›†æˆé¢œè‰²æ¨ç†æ¨¡å‹"""
    
    def __init__(self, num_classes=5):
        super().__init__()
        
        # ä¸»è¦é¢œè‰²ç»„åˆåˆ†ç±»å™¨
        self.color_classifier = ImprovedColorCombinationHead(num_classes=num_classes)
        
        # é¢œè‰²å…³ç³»æ¨ç†æ¨¡å—
        self.relation_reasoner = ColorRelationReasoning()
        
        # é›†æˆèåˆå±‚
        self.integration_layer = nn.Sequential(
            nn.Linear(num_classes + 5, 64),  # åˆ†ç±»å™¨è¾“å‡º + å…³ç³»æ¨ç†è¾“å‡º
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, num_classes)
        )
        
        # æ³¨æ„åŠ›æƒé‡
        self.classifier_weight = nn.Parameter(torch.tensor(0.7))
        self.reasoner_weight = nn.Parameter(torch.tensor(0.3))
        
        # å…³ç³»ä¸€è‡´æ€§æŸå¤±æƒé‡
        self.relation_consistency_weight = 0.1
        
    def forward(self, x: torch.Tensor) -> Dict:
        """å‰å‘ä¼ æ’­"""
        # 1. ä¸»è¦é¢œè‰²åˆ†ç±»å™¨
        classifier_outputs = self.color_classifier(x)
        classifier_logits = classifier_outputs['combination_logits']
        classifier_probs = F.softmax(classifier_logits, dim=-1)
        
        # 2. é¢œè‰²å…³ç³»æ¨ç†
        # æå–é¢œè‰²ç‰¹å¾ç”¨äºå…³ç³»æ¨ç†
        color_features = self._extract_color_features(x)
        relation_outputs = self.relation_reasoner(color_features)
        relation_logits = relation_outputs['combination_logits']
        relation_probs = F.softmax(relation_logits, dim=-1)
        
        # 3. é›†æˆèåˆ
        combined_features = torch.cat([classifier_probs, relation_probs], dim=-1)
        integrated_logits = self.integration_layer(combined_features)
        integrated_probs = F.softmax(integrated_logits, dim=-1)
        
        # 4. åŠ æƒèåˆ
        final_logits = (self.classifier_weight * classifier_logits + 
                       self.reasoner_weight * relation_logits)
        final_probs = F.softmax(final_logits, dim=-1)
        
        return {
            'classifier_logits': classifier_logits,
            'classifier_probs': classifier_probs,
            'relation_logits': relation_logits,
            'relation_probs': relation_probs,
            'integrated_logits': integrated_logits,
            'integrated_probs': integrated_probs,
            'final_logits': final_logits,
            'final_probs': final_probs,
            'relation_features': relation_outputs['relation_features'],
            'color_semantic': classifier_outputs['color_semantic']
        }
    
    def _extract_color_features(self, x: torch.Tensor) -> torch.Tensor:
        """æå–é¢œè‰²ç‰¹å¾ç”¨äºå…³ç³»æ¨ç†"""
        batch_size = x.size(0)
        
        # ç®€åŒ–çš„é¢œè‰²ç‰¹å¾æå–
        # è®¡ç®—æ¯ä¸ªé€šé“çš„å¹³å‡å€¼ä½œä¸ºé¢œè‰²ç‰¹å¾
        color_features = torch.mean(x, dim=(2, 3))  # [batch, 3]
        
        # æ‰©å±•åˆ°8ç»´ï¼ˆå¯¹åº”8ç§é¢œè‰²ï¼‰
        expanded_features = torch.zeros(batch_size, 8, device=x.device)
        
        # åŸºäºRGBå€¼æ˜ å°„åˆ°é¢œè‰²ç‰¹å¾
        for i in range(batch_size):
            r, g, b = color_features[i]
            
            # ç®€å•çš„é¢œè‰²æ˜ å°„é€»è¾‘
            if r > 0.5 and g < 0.3 and b < 0.3:
                expanded_features[i, 0] = 1.0  # red
            elif r > 0.5 and g > 0.3 and b < 0.3:
                expanded_features[i, 1] = 1.0  # orange
            elif r > 0.5 and g > 0.5 and b < 0.3:
                expanded_features[i, 2] = 1.0  # yellow
            elif r < 0.3 and g > 0.5 and b < 0.3:
                expanded_features[i, 3] = 1.0  # green
            elif r < 0.3 and g < 0.3 and b > 0.5:
                expanded_features[i, 4] = 1.0  # blue
            elif r > 0.5 and g < 0.3 and b > 0.5:
                expanded_features[i, 5] = 1.0  # purple
            elif r > 0.7 and g > 0.7 and b > 0.7:
                expanded_features[i, 6] = 1.0  # white
            elif r < 0.3 and g < 0.3 and b < 0.3:
                expanded_features[i, 7] = 1.0  # black
            else:
                # é»˜è®¤æ˜ å°„åˆ°ä¸»è¦é¢œè‰²
                max_val, max_idx = torch.max(color_features[i], 0)
                expanded_features[i, max_idx] = 1.0
        
        return expanded_features
    
    def compute_integrated_loss(self, outputs: Dict, targets: torch.Tensor) -> Dict:
        """è®¡ç®—é›†æˆæŸå¤±"""
        # 1. åˆ†ç±»å™¨æŸå¤±
        classifier_loss = F.cross_entropy(
            outputs['classifier_logits'], 
            targets, 
            label_smoothing=0.1
        )
        
        # 2. å…³ç³»æ¨ç†æŸå¤±
        relation_loss = F.cross_entropy(
            outputs['relation_logits'],
            targets,
            label_smoothing=0.1
        )
        
        # 3. é›†æˆæŸå¤±
        integrated_loss = F.cross_entropy(
            outputs['integrated_logits'],
            targets,
            label_smoothing=0.1
        )
        
        # 4. æœ€ç»ˆèåˆæŸå¤±
        final_loss = F.cross_entropy(
            outputs['final_logits'],
            targets,
            label_smoothing=0.1
        )
        
        # 5. å…³ç³»ä¸€è‡´æ€§æŸå¤±
        consistency_loss = self._compute_relation_consistency_loss(outputs)
        
        # 6. æ€»æŸå¤±
        total_loss = (classifier_loss + 
                     0.5 * relation_loss + 
                     0.3 * integrated_loss + 
                     final_loss + 
                     self.relation_consistency_weight * consistency_loss)
        
        return {
            'total_loss': total_loss,
            'classifier_loss': classifier_loss,
            'relation_loss': relation_loss,
            'integrated_loss': integrated_loss,
            'final_loss': final_loss,
            'consistency_loss': consistency_loss
        }
    
    def _compute_relation_consistency_loss(self, outputs: Dict) -> torch.Tensor:
        """è®¡ç®—å…³ç³»ä¸€è‡´æ€§æŸå¤±"""
        # é¼“åŠ±åˆ†ç±»å™¨å’Œå…³ç³»æ¨ç†å™¨çš„ä¸€è‡´æ€§
        classifier_probs = outputs['classifier_probs']
        relation_probs = outputs['relation_probs']
        
        # KLæ•£åº¦æŸå¤±
        kl_loss = F.kl_div(
            torch.log(relation_probs + 1e-8),
            classifier_probs,
            reduction='batchmean'
        )
        
        return kl_loss

def train_integrated_model(model: IntegratedColorReasoningModel,
                         train_loader,
                         val_loader,
                         epochs: int = 30,
                         lr: float = 0.0005) -> Dict:
    """è®­ç»ƒé›†æˆæ¨¡å‹"""
    print("ğŸ¨ å¼€å§‹è®­ç»ƒé›†æˆé¢œè‰²æ¨ç†æ¨¡å‹...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_accuracy': [],
        'val_accuracy': [],
        'classifier_accuracy': [],
        'relation_accuracy': [],
        'integrated_accuracy': []
    }
    
    best_accuracy = 0.0
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(device), targets.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(data)
            loss_dict = model.compute_integrated_loss(outputs, targets)
            
            # åå‘ä¼ æ’­
            loss_dict['total_loss'].backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # ç»Ÿè®¡
            train_loss += loss_dict['total_loss'].item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = outputs['final_logits'].max(1)
            train_total += targets.size(0)
            train_correct += predicted.eq(targets).sum().item()
            
            if batch_idx % 30 == 0:
                print(f"  Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, "
                      f"Loss: {loss_dict['total_loss'].item():.4f}")
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        classifier_correct = 0
        relation_correct = 0
        integrated_correct = 0
        
        with torch.no_grad():
            for data, targets in val_loader:
                data, targets = data.to(device), targets.to(device)
                
                outputs = model(data)
                loss_dict = model.compute_integrated_loss(outputs, targets)
                
                val_loss += loss_dict['total_loss'].item()
                
                # å„ç§å‡†ç¡®ç‡
                _, final_pred = outputs['final_logits'].max(1)
                _, classifier_pred = outputs['classifier_logits'].max(1)
                _, relation_pred = outputs['relation_logits'].max(1)
                _, integrated_pred = outputs['integrated_logits'].max(1)
                
                val_total += targets.size(0)
                val_correct += final_pred.eq(targets).sum().item()
                classifier_correct += classifier_pred.eq(targets).sum().item()
                relation_correct += relation_pred.eq(targets).sum().item()
                integrated_correct += integrated_pred.eq(targets).sum().item()
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_accuracy = 100.0 * train_correct / train_total
        val_accuracy = 100.0 * val_correct / val_total
        classifier_accuracy = 100.0 * classifier_correct / val_total
        relation_accuracy = 100.0 * relation_correct / val_total
        integrated_accuracy = 100.0 * integrated_correct / val_total
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss / len(train_loader))
        history['val_loss'].append(val_loss / len(val_loader))
        history['train_accuracy'].append(train_accuracy)
        history['val_accuracy'].append(val_accuracy)
        history['classifier_accuracy'].append(classifier_accuracy)
        history['relation_accuracy'].append(relation_accuracy)
        history['integrated_accuracy'].append(integrated_accuracy)
        
        if epoch % 5 == 0:
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Acc: {train_accuracy:.2f}%, "
                  f"Val Acc: {val_accuracy:.2f}%, "
                  f"Classifier: {classifier_accuracy:.2f}%, "
                  f"Relation: {relation_accuracy:.2f}%, "
                  f"Integrated: {integrated_accuracy:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model.state_dict(), 'best_integrated_color_model.pth')
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.2f}%")
    return history

def test_integrated_model(model: IntegratedColorReasoningModel, test_loader):
    """æµ‹è¯•é›†æˆæ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•é›†æˆé¢œè‰²æ¨ç†æ¨¡å‹...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    results = {
        'final_accuracy': 0.0,
        'classifier_accuracy': 0.0,
        'relation_accuracy': 0.0,
        'integrated_accuracy': 0.0,
        'detailed_results': []
    }
    
    final_correct = 0
    classifier_correct = 0
    relation_correct = 0
    integrated_correct = 0
    total_samples = 0
    
    with torch.no_grad():
        for data, targets in test_loader:
            data, targets = data.to(device), targets.to(device)
            
            outputs = model(data)
            
            # è®¡ç®—å„ç§å‡†ç¡®ç‡
            _, final_pred = outputs['final_logits'].max(1)
            _, classifier_pred = outputs['classifier_logits'].max(1)
            _, relation_pred = outputs['relation_logits'].max(1)
            _, integrated_pred = outputs['integrated_logits'].max(1)
            
            total_samples += targets.size(0)
            final_correct += final_pred.eq(targets).sum().item()
            classifier_correct += classifier_pred.eq(targets).sum().item()
            relation_correct += relation_pred.eq(targets).sum().item()
            integrated_correct += integrated_pred.eq(targets).sum().item()
            
            # è®°å½•è¯¦ç»†ç»“æœ
            for i in range(targets.size(0)):
                results['detailed_results'].append({
                    'target': targets[i].item(),
                    'final_pred': final_pred[i].item(),
                    'classifier_pred': classifier_pred[i].item(),
                    'relation_pred': relation_pred[i].item(),
                    'integrated_pred': integrated_pred[i].item(),
                    'final_correct': final_pred[i].eq(targets[i]).item(),
                    'classifier_correct': classifier_pred[i].eq(targets[i]).item(),
                    'relation_correct': relation_pred[i].eq(targets[i]).item(),
                    'integrated_correct': integrated_pred[i].eq(targets[i]).item()
                })
    
    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
    results['final_accuracy'] = 100.0 * final_correct / total_samples
    results['classifier_accuracy'] = 100.0 * classifier_correct / total_samples
    results['relation_accuracy'] = 100.0 * relation_correct / total_samples
    results['integrated_accuracy'] = 100.0 * integrated_correct / total_samples
    
    print(f"ğŸ¯ æµ‹è¯•ç»“æœ:")
    print(f"  æœ€ç»ˆå‡†ç¡®ç‡: {results['final_accuracy']:.2f}%")
    print(f"  åˆ†ç±»å™¨å‡†ç¡®ç‡: {results['classifier_accuracy']:.2f}%")
    print(f"  å…³ç³»æ¨ç†å‡†ç¡®ç‡: {results['relation_accuracy']:.2f}%")
    print(f"  é›†æˆå‡†ç¡®ç‡: {results['integrated_accuracy']:.2f}%")
    
    return results

if __name__ == "__main__":
    # åˆ›å»ºé›†æˆæ¨¡å‹
    model = IntegratedColorReasoningModel(num_classes=5)
    
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†ç±»å™¨æƒé‡
    try:
        model.color_classifier.load_state_dict(
            torch.load('best_improved_color_combination_head.pth', map_location='cpu')
        )
        print("âœ… åŠ è½½äº†é¢„è®­ç»ƒçš„åˆ†ç±»å™¨æƒé‡")
    except:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # åŠ è½½é¢„è®­ç»ƒçš„å…³ç³»æ¨ç†æƒé‡
    try:
        model.relation_reasoner.load_state_dict(
            torch.load('best_color_relation_model.pth', map_location='cpu')
        )
        print("âœ… åŠ è½½äº†é¢„è®­ç»ƒçš„å…³ç³»æ¨ç†æƒé‡")
    except:
        print("âš ï¸ æœªæ‰¾åˆ°å…³ç³»æ¨ç†æƒé‡ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„GTSRBæ•°æ®åŠ è½½å™¨ï¼‰
    from improved_color_combination import BalancedColorCombinationDataset
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = BalancedColorCombinationDataset(size=1000, use_real_data=True)
    val_dataset = BalancedColorCombinationDataset(size=200, use_real_data=True)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # è®­ç»ƒæ¨¡å‹
    history = train_integrated_model(model, train_loader, val_loader, epochs=20)
    
    # æµ‹è¯•æ¨¡å‹
    results = test_integrated_model(model, val_loader)
    
    # ä¿å­˜ç»“æœ
    with open('integrated_color_reasoning_results.json', 'w') as f:
        json.dump({
            'history': history,
            'test_results': results
        }, f, indent=2)
    
    print("âœ… é›†æˆé¢œè‰²æ¨ç†æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼")
