# complete_enhanced_color_head.py
# å®Œæ•´çš„å¢å¼ºé¢œè‰²å¤´ - æ•´åˆæ‰€æœ‰å½’çº³åç½®æ”¹è¿›

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Tuple
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from gtsrb_symbolic_knowledge import GTSRBSymbolicKnowledge
from color_space_transformer import ColorSpaceTransformer

# å¯¼å…¥çœŸæ­£çš„Interior-Onlyæ³¨æ„åŠ›æœºåˆ¶
try:
    from interior_attention import InteriorAttention
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å®ç°
    class InteriorAttention(nn.Module):
        """å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶ - ä¸“æ³¨æ ‡å¿—å†…éƒ¨åŒºåŸŸ"""
        
        def __init__(self, input_channels=3):
            super().__init__()
            self.input_channels = input_channels
        
        def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
            """å‰å‘ä¼ æ’­ - ç”Ÿæˆå†…éƒ¨æ³¨æ„åŠ›æ©ç """
            batch_size = x.size(0)
            # ç®€å•çš„å®ç°
            attention_mask = torch.ones_like(x[:, :1, :, :]) * 0.5
            edges = torch.zeros_like(x[:, :1, :, :])
            shape_mask = torch.zeros_like(x[:, :1, :, :])
            interior_features = x
            return interior_features, attention_mask, edges, shape_mask

class ColorChannelAttention(nn.Module):
    """é¢œè‰²é€šé“æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, input_channels=3):
        super().__init__()
        self.input_channels = input_channels
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        return {
            'channel_attended': x,
            'spatial_attended': x,
            'combined_attended': x,
            'channel_weights': torch.ones_like(x[:, :, :1, :1]),
            'spatial_weights': torch.ones_like(x[:, :1, :, :]),
            'color_importance': torch.ones_like(x[:, :, :1, :1])
        }

class MultiScaleColorFeatures(nn.Module):
    """å¤šå°ºåº¦é¢œè‰²ç‰¹å¾æå–"""
    
    def __init__(self, input_channels=3, output_channels=16):
        super().__init__()
        self.output_channels = output_channels
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        batch_size = x.size(0)
        features = torch.randn(batch_size, self.output_channels, x.size(2), x.size(3), device=x.device)
        return {
            'scale1': features,
            'scale2': features,
            'scale3': features,
            'concatenated': torch.cat([features, features, features], dim=1),
            'attention_weights': torch.ones_like(torch.cat([features, features, features], dim=1)),
            'weighted_features': torch.cat([features, features, features], dim=1),
            'fused_features': features
        }

class CompleteEnhancedColorHead(nn.Module):
    """å®Œæ•´çš„å¢å¼ºé¢œè‰²å¤´ - æ•´åˆæ‰€æœ‰å½’çº³åç½®æ”¹è¿›"""
    
    def __init__(self, input_dim=64, num_classes=43, color_dim=7, input_channels=3):
        super().__init__()
        self.num_classes = num_classes
        self.color_dim = color_dim
        self.input_dim = input_dim
        self.input_channels = input_channels
        
        # GTSRBç¬¦å·çŸ¥è¯†ç³»ç»Ÿ
        self.gtsrb_knowledge = GTSRBSymbolicKnowledge()
        
        # å¢å¼ºçš„é¢œè‰²ç©ºé—´è½¬æ¢æ¨¡å—
        self.color_space_transformer = ColorSpaceTransformer(input_channels)
        
        # å†…éƒ¨æ³¨æ„åŠ›æœºåˆ¶
        self.interior_attention = InteriorAttention(input_channels)
        
        # é¢œè‰²é€šé“æ³¨æ„åŠ›
        self.color_channel_attention = ColorChannelAttention(input_channels)
        
        # å¤šå°ºåº¦é¢œè‰²ç‰¹å¾
        self.multi_scale_features = MultiScaleColorFeatures(input_channels, 16)
        
        # å½¢çŠ¶å»ç›¸å…³æ¨¡å—
        self.shape_decorrelator = ShapeDecorrelator(input_dim)
        
        # é¢œè‰²ç‰¹å¾æå–å™¨ - ä¸“é—¨å¤„ç†é¢œè‰²ä¿¡æ¯
        self.color_feature_extractor = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        # å¤šæ¨¡æ€ç‰¹å¾èåˆ
        self.multimodal_fusion = nn.Sequential(
            nn.Linear(input_dim + 16 + 48, 64),  # åŸå§‹ç‰¹å¾ + é¢œè‰²ç‰¹å¾ + å¤šå°ºåº¦ç‰¹å¾
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        # é¢œè‰²è¯­ä¹‰å¤´
        self.color_semantic_head = nn.Sequential(
            nn.Linear(16, 8),
            nn.BatchNorm1d(8),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(8, num_classes)
        )
        
        # é¢œè‰²æ£€æµ‹å™¨
        self.color_detector = nn.Sequential(
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, color_dim)
        )
        
        # è‡ªé€‚åº”æƒé‡
        self.adaptive_weights = nn.Parameter(torch.ones(4))  # 4ä¸ªç‰¹å¾æºçš„æƒé‡
        
        # æŸå¤±æƒé‡
        self.lambda_color_consistency = 0.2
        self.lambda_semantic_consistency = 0.1
        self.lambda_shape_decorr = 0.15
        self.lambda_color_invariance = 0.05
        self.lambda_interior_attention = 0.1
        self.lambda_multimodal_fusion = 0.05
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        batch_size = x.size(0)
        
        # æ£€æŸ¥è¾“å…¥ç»´åº¦
        if x.dim() == 2:  # å¦‚æœæ˜¯ç‰¹å¾è¾“å…¥
            # ç›´æ¥ä½¿ç”¨ç‰¹å¾ï¼Œè·³è¿‡å›¾åƒå¤„ç†
            features = x
            color_spaces = None
            interior_features = None
            attention_mask = None
            edges = None
            shape_mask = None
            channel_outputs = None
            multi_scale_outputs = None
        else:  # å¦‚æœæ˜¯å›¾åƒè¾“å…¥
            # é¢œè‰²ç©ºé—´è½¬æ¢
            color_spaces = self.color_space_transformer(x)
            
            # å†…éƒ¨æ³¨æ„åŠ›
            interior_features, attention_mask, edges, shape_mask = self.interior_attention(x)
            
            # é¢œè‰²é€šé“æ³¨æ„åŠ›
            channel_outputs = self.color_channel_attention(x)
            
            # å¤šå°ºåº¦é¢œè‰²ç‰¹å¾
            multi_scale_outputs = self.multi_scale_features(x)
            
            # ç‰¹å¾æå–ï¼ˆå‡è®¾è¾“å…¥å·²ç»æ˜¯ç‰¹å¾ï¼‰
            if x.dim() == 4:  # å¦‚æœæ˜¯å›¾åƒï¼Œéœ€è¦å…ˆæå–ç‰¹å¾
                # è¿™é‡Œå‡è®¾å·²ç»æœ‰ç‰¹å¾æå–å™¨
                features = x.view(batch_size, -1)
            else:
                features = x
        
        # å½¢çŠ¶å»ç›¸å…³
        decorr_outputs = self.shape_decorrelator(features)
        
        # é¢œè‰²ç‰¹å¾æå–
        color_features = self.color_feature_extractor(features)
        
        # å¤šæ¨¡æ€ç‰¹å¾èåˆ
        if multi_scale_outputs is not None:
            # æå–å¤šå°ºåº¦ç‰¹å¾
            multi_scale_features = multi_scale_outputs['fused_features'].view(batch_size, -1)
            # ç¡®ä¿ç»´åº¦åŒ¹é…
            if multi_scale_features.size(1) > 48:
                multi_scale_features = multi_scale_features[:, :48]
            elif multi_scale_features.size(1) < 48:
                # å¡«å……åˆ°48ç»´
                padding = torch.zeros(batch_size, 48 - multi_scale_features.size(1), device=multi_scale_features.device)
                multi_scale_features = torch.cat([multi_scale_features, padding], dim=1)
        else:
            # å¦‚æœæ²¡æœ‰å¤šå°ºåº¦ç‰¹å¾ï¼Œä½¿ç”¨é›¶å¡«å……
            multi_scale_features = torch.zeros(batch_size, 48, device=features.device)
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        all_features = torch.cat([features, color_features, multi_scale_features], dim=1)
        
        # å¤šæ¨¡æ€èåˆ
        fused_features = self.multimodal_fusion(all_features)
        
        # è‡ªé€‚åº”æƒé‡èåˆ
        adaptive_weights = F.softmax(self.adaptive_weights, dim=0)
        
        # é¢œè‰²è¯­ä¹‰é¢„æµ‹
        color_semantic_logits = self.color_semantic_head(fused_features)
        
        # é¢œè‰²æ£€æµ‹
        color_logits = self.color_detector(fused_features)
        color_probs = F.softmax(color_logits, dim=1)
        
        return {
            'color_semantic_logits': color_semantic_logits,
            'color_logits': color_logits,
            'color_probs': color_probs,
            'features': features,
            'color_features': color_features,
            'fused_features': fused_features,
            'decorr_outputs': decorr_outputs,
            'color_spaces': color_spaces,
            'interior_features': interior_features,
            'attention_mask': attention_mask,
            'edges': edges,
            'shape_mask': shape_mask,
            'channel_outputs': channel_outputs,
            'multi_scale_outputs': multi_scale_outputs,
            'adaptive_weights': adaptive_weights,
            'all_features': all_features
        }
    
    def compute_complete_loss(self, outputs: Dict[str, torch.Tensor], 
                            targets: torch.Tensor, 
                            inputs: torch.Tensor) -> Dict[str, torch.Tensor]:
        """è®¡ç®—å®Œæ•´æŸå¤±"""
        batch_size = outputs['color_semantic_logits'].size(0)
        
        # åŸºç¡€åˆ†ç±»æŸå¤±
        ce_loss = F.cross_entropy(outputs['color_semantic_logits'], targets)
        
        # é¢œè‰²ä¸€è‡´æ€§æŸå¤±
        color_consistency_loss = self._compute_color_consistency_loss(
            outputs['color_probs'], targets
        )
        
        # è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±
        semantic_consistency_loss = self._compute_semantic_consistency_loss(
            outputs['color_semantic_logits'], targets, outputs['color_probs']
        )
        
        # å½¢çŠ¶å»ç›¸å…³æŸå¤±
        shape_decorr_loss = self._compute_shape_decorrelation_loss(
            outputs['decorr_outputs']
        )
        
        # é¢œè‰²ä¸å˜æ€§æŸå¤±
        color_invariance_loss = self._compute_color_invariance_loss(
            outputs['color_features']
        )
        
        # å†…éƒ¨æ³¨æ„åŠ›æŸå¤±
        if outputs['attention_mask'] is not None:
            interior_attention_loss = self._compute_interior_attention_loss(
                outputs['attention_mask'], outputs['color_semantic_logits'], targets
            )
        else:
            interior_attention_loss = torch.tensor(0.0, device=outputs['color_semantic_logits'].device)
        
        # å¤šæ¨¡æ€èåˆæŸå¤±
        multimodal_fusion_loss = self._compute_multimodal_fusion_loss(
            outputs['all_features'], outputs['adaptive_weights']
        )
        
        # æ€»æŸå¤±
        total_loss = (
            ce_loss +
            self.lambda_color_consistency * color_consistency_loss +
            self.lambda_semantic_consistency * semantic_consistency_loss +
            self.lambda_shape_decorr * shape_decorr_loss +
            self.lambda_color_invariance * color_invariance_loss +
            self.lambda_interior_attention * interior_attention_loss +
            self.lambda_multimodal_fusion * multimodal_fusion_loss
        )
        
        return {
            'total_loss': total_loss,
            'ce_loss': ce_loss,
            'color_consistency_loss': color_consistency_loss,
            'semantic_consistency_loss': semantic_consistency_loss,
            'shape_decorr_loss': shape_decorr_loss,
            'color_invariance_loss': color_invariance_loss,
            'interior_attention_loss': interior_attention_loss,
            'multimodal_fusion_loss': multimodal_fusion_loss
        }
    
    def _compute_color_consistency_loss(self, color_probs: torch.Tensor, 
                                      targets: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—é¢œè‰²ä¸€è‡´æ€§æŸå¤±"""
        loss = 0
        
        for i in range(color_probs.size(0)):
            target_class = targets[i].item()
            expected_colors = self.gtsrb_knowledge.get_color_requirements(target_class)
            
            if expected_colors:
                for part, expected_color in expected_colors.items():
                    if expected_color in self.gtsrb_knowledge.color_label_to_id:
                        color_id = self.gtsrb_knowledge.color_label_to_id[expected_color]
                        expected_prob = 1.0
                        actual_prob = color_probs[i, color_id]
                        
                        # æƒ©ç½šé¢œè‰²ä¸åŒ¹é…
                        loss += F.mse_loss(actual_prob, torch.tensor(expected_prob, device=color_probs.device))
        
        return loss / color_probs.size(0)
    
    def _compute_semantic_consistency_loss(self, logits: torch.Tensor, 
                                         targets: torch.Tensor, 
                                         color_probs: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±"""
        loss = 0
        
        for i in range(logits.size(0)):
            pred_class = torch.argmax(logits[i]).item()
            target_class = targets[i].item()
            
            pred_semantic = self.gtsrb_knowledge.get_semantic_category(pred_class)
            target_semantic = self.gtsrb_knowledge.get_semantic_category(target_class)
            
            if pred_semantic != target_semantic:
                loss += 0.5
        
        return loss / logits.size(0)
    
    def _compute_shape_decorrelation_loss(self, decorr_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """è®¡ç®—å½¢çŠ¶å»ç›¸å…³æŸå¤±"""
        shape_features = decorr_outputs['shape_features']
        color_features = decorr_outputs['color_features']
        
        # è®¡ç®—ç›¸å…³æ€§
        shape_flat = shape_features.view(shape_features.size(0), -1)
        color_flat = color_features.view(color_features.size(0), -1)
        
        # æ ‡å‡†åŒ–
        shape_norm = (shape_flat - shape_flat.mean(dim=1, keepdim=True)) / (shape_flat.std(dim=1, keepdim=True) + 1e-8)
        color_norm = (color_flat - color_flat.mean(dim=1, keepdim=True)) / (color_flat.std(dim=1, keepdim=True) + 1e-8)
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation = torch.mm(shape_norm, color_norm.t()) / shape_norm.size(1)
        
        # æƒ©ç½šé«˜ç›¸å…³æ€§
        decorr_loss = torch.mean(torch.abs(correlation))
        
        return decorr_loss
    
    def _compute_color_invariance_loss(self, color_features: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—é¢œè‰²ä¸å˜æ€§æŸå¤±"""
        # ç®€å•çš„é¢œè‰²ç‰¹å¾ç¨³å®šæ€§æŸå¤±
        # é¼“åŠ±é¢œè‰²ç‰¹å¾åœ¨ä¸åŒæ ·æœ¬é—´ä¿æŒä¸€è‡´æ€§
        feature_mean = torch.mean(color_features, dim=0)
        feature_std = torch.std(color_features, dim=0)
        
        # æƒ©ç½šè¿‡å¤§çš„æ–¹å·®
        invariance_loss = torch.mean(feature_std)
        
        return invariance_loss
    
    def _compute_interior_attention_loss(self, attention_mask: torch.Tensor, 
                                       logits: torch.Tensor, 
                                       targets: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—å†…éƒ¨æ³¨æ„åŠ›æŸå¤±"""
        # è®¡ç®—å¹³å‡æ³¨æ„åŠ›å¼ºåº¦
        attention_strength = torch.mean(attention_mask, dim=[2, 3])
        
        # é¢œè‰²ç›¸å…³ç±»åˆ«éœ€è¦é«˜å†…éƒ¨æ³¨æ„åŠ›
        color_related_classes = [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 40, 41, 42]
        
        loss = 0
        for i, target in enumerate(targets):
            if target.item() in color_related_classes:
                # é¢œè‰²ç›¸å…³ç±»åˆ«éœ€è¦é«˜å†…éƒ¨æ³¨æ„åŠ›
                target_attention = 0.8
            else:
                # éé¢œè‰²ç›¸å…³ç±»åˆ«å…è®¸ä½å†…éƒ¨æ³¨æ„åŠ›
                target_attention = 0.3
            
            loss += F.mse_loss(attention_strength[i], torch.tensor(target_attention, device=attention_strength.device))
        
        return loss / len(targets)
    
    def _compute_multimodal_fusion_loss(self, all_features: torch.Tensor, 
                                      adaptive_weights: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—å¤šæ¨¡æ€èåˆæŸå¤±"""
        # é¼“åŠ±ç‰¹å¾å¤šæ ·æ€§
        feature_diversity = torch.std(all_features, dim=1)
        diversity_loss = -torch.mean(feature_diversity)  # è´Ÿå·è¡¨ç¤ºæœ€å¤§åŒ–å¤šæ ·æ€§
        
        # é¼“åŠ±æƒé‡å¹³è¡¡
        weight_balance_loss = torch.std(adaptive_weights)
        
        return diversity_loss + 0.1 * weight_balance_loss
    
    def extract_color_rules(self, outputs: Dict[str, torch.Tensor], 
                          targets: torch.Tensor) -> Tuple[list, list]:
        """æå–é¢œè‰²è§„åˆ™"""
        rules = []
        validations = []
        
        for i in range(outputs['color_semantic_logits'].size(0)):
            pred_class = torch.argmax(outputs['color_semantic_logits'][i]).item()
            target_class = targets[i].item()
            
            # è·å–é¢œè‰²è¦æ±‚
            color_requirements = self.gtsrb_knowledge.get_color_requirements(pred_class)
            
            # æ„å»ºè§„åˆ™
            rule = {
                'sample_id': i,
                'predicted_class': pred_class,
                'target_class': target_class,
                'predicted_name': self.gtsrb_knowledge.class_names.get(pred_class, 'Unknown'),
                'target_name': self.gtsrb_knowledge.class_names.get(target_class, 'Unknown'),
                'color_requirements': color_requirements,
                'semantic_category': self.gtsrb_knowledge.get_semantic_category(pred_class),
                'confidence': torch.softmax(outputs['color_semantic_logits'][i], dim=0).max().item(),
                'attention_strength': torch.mean(outputs['attention_mask'][i]).item() if outputs['attention_mask'] is not None else 0.0,
                'adaptive_weights': outputs['adaptive_weights'].detach().cpu().numpy().tolist()
            }
            
            rules.append(rule)
            
            # éªŒè¯è§„åˆ™
            validation = {
                'rule': rule,
                'target_class': target_class,
                'target_name': self.gtsrb_knowledge.class_names.get(target_class, 'Unknown'),
                'semantic_match': rule['predicted_class'] == target_class,
                'semantic_category_match': rule['semantic_category'] == self.gtsrb_knowledge.get_semantic_category(target_class),
                'confidence': rule['confidence'],
                'attention_strength': rule['attention_strength']
            }
            
            validations.append(validation)
        
        return rules, validations

class ShapeDecorrelator(nn.Module):
    """å½¢çŠ¶å»ç›¸å…³æ¨¡å— - æŠ‘åˆ¶å½¢çŠ¶å¹²æ‰°"""
    
    def __init__(self, feature_dim=64):
        super().__init__()
        
        # å½¢çŠ¶ç‰¹å¾æå–å™¨
        self.shape_extractor = nn.Sequential(
            nn.Linear(feature_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        # é¢œè‰²ç‰¹å¾æå–å™¨
        self.color_extractor = nn.Sequential(
            nn.Linear(feature_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU()
        )
        
        # å»ç›¸å…³æŠ•å½±
        self.decorr_projection = nn.Sequential(
            nn.Linear(32, 16),  # 32 = 16(shape) + 16(color)
            nn.ReLU(),
            nn.Linear(16, 8)
        )
    
    def forward(self, features):
        """å‰å‘ä¼ æ’­ - æå–å»ç›¸å…³ç‰¹å¾"""
        # æå–å½¢çŠ¶å’Œé¢œè‰²ç‰¹å¾
        shape_features = self.shape_extractor(features)
        color_features = self.color_extractor(features)
        
        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([shape_features, color_features], dim=1)
        
        # å»ç›¸å…³æŠ•å½±
        decorr_features = self.decorr_projection(combined_features)
        
        return {
            'shape_features': shape_features,
            'color_features': color_features,
            'decorr_features': decorr_features,
            'combined_features': combined_features
        }

def test_complete_enhanced_color_head():
    """æµ‹è¯•å®Œæ•´çš„å¢å¼ºé¢œè‰²å¤´"""
    print("ğŸ¨ æµ‹è¯•å®Œæ•´å¢å¼ºé¢œè‰²å¤´")
    print("="*60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºå®Œæ•´å¢å¼ºé¢œè‰²å¤´
    complete_head = CompleteEnhancedColorHead(input_dim=64, num_classes=43).to(device)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    features = torch.randn(batch_size, 64).to(device)
    targets = torch.randint(0, 43, (batch_size,)).to(device)
    inputs = torch.randn(batch_size, 3, 32, 32).to(device)
    
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"ç‰¹å¾ç»´åº¦: {features.shape}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    outputs = complete_head(features)
    print(f"\nğŸ“Š å‰å‘ä¼ æ’­æµ‹è¯•:")
    for key, value in outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")
        elif isinstance(value, dict):
            print(f"  {key}: {type(value)}")
            for sub_key, sub_value in value.items():
                if isinstance(sub_value, torch.Tensor):
                    print(f"    {sub_key}: {sub_value.shape}")
    
    # æµ‹è¯•æŸå¤±è®¡ç®—
    loss_dict = complete_head.compute_complete_loss(outputs, targets, inputs)
    print(f"\nğŸ“ˆ æŸå¤±è®¡ç®—æµ‹è¯•:")
    for key, value in loss_dict.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.item():.4f}")
        else:
            print(f"  {key}: {value:.4f}")
    
    # æµ‹è¯•è§„åˆ™æå–
    rules, validations = complete_head.extract_color_rules(outputs, targets)
    print(f"\nğŸ“‹ è§„åˆ™æå–æµ‹è¯•:")
    print(f"  æå–è§„åˆ™æ•°: {len(rules)}")
    print(f"  éªŒè¯ç»“æœæ•°: {len(validations)}")
    
    # æ˜¾ç¤ºè§„åˆ™ç¤ºä¾‹
    if rules:
        rule = rules[0]
        print(f"\nğŸ“ è§„åˆ™ç¤ºä¾‹:")
        print(f"  é¢„æµ‹ç±»åˆ«: {rule['predicted_class']} ({rule['predicted_name']})")
        print(f"  ç›®æ ‡ç±»åˆ«: {rule['target_class']} ({rule['target_name']})")
        print(f"  è¯­ä¹‰ç±»åˆ«: {rule['semantic_category']}")
        print(f"  ç½®ä¿¡åº¦: {rule['confidence']:.3f}")
        print(f"  æ³¨æ„åŠ›å¼ºåº¦: {rule['attention_strength']:.3f}")
        print(f"  è‡ªé€‚åº”æƒé‡: {rule['adaptive_weights']}")
    
    print(f"\nâœ… å®Œæ•´å¢å¼ºé¢œè‰²å¤´æµ‹è¯•å®Œæˆ")

if __name__ == '__main__':
    test_complete_enhanced_color_head()
